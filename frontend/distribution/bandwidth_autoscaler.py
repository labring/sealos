#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import asyncio
import subprocess
from datetime import datetime
from kubernetes import client, config
import logging
from typing import Dict, List

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('bandwidth_autoscaler.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class BandwidthAutoscaler:
    def __init__(self):
        """åˆå§‹åŒ–åŸºäºå¸¦å®½çš„è‡ªåŠ¨æ‰©ç¼©å®¹æ§åˆ¶å™¨"""
        try:
            # å°è¯•åŠ è½½é›†ç¾¤å†…é…ç½®ï¼Œå¦‚æœå¤±è´¥åˆ™ä½¿ç”¨æœ¬åœ°é…ç½®
            config.load_incluster_config()
        except:
            try:
                # å°è¯•ä½¿ç”¨é»˜è®¤kubeconfig
                config.load_kube_config()
            except:
                try:
                    # å°è¯•ä½¿ç”¨å®¹å™¨ä¸­çš„kubeconfigè·¯å¾„
                    config.load_kube_config(config_file="/etc/kubernetes/admin.conf")
                except:
                    logger.error("æ— æ³•åŠ è½½k8sé…ç½®ï¼Œè¯·æ£€æŸ¥é›†ç¾¤è¿æ¥")
                    raise Exception("K8sé…ç½®åŠ è½½å¤±è´¥")
        
        self.v1 = client.AppsV1Api()
        self.core_v1 = client.CoreV1Api()
        
        # æ‰©ç¼©å®¹å†å²è®°å½•ï¼Œç”¨äºé˜²æ­¢é¢‘ç¹å˜åŒ–
        self.scale_history = []
        self.cooldown_period = 120  # å†·å´æœŸ2åˆ†é’Ÿ
        self.sampling_interval = 2.0  # å¸¦å®½é‡‡æ ·é—´éš”ï¼ˆç§’ï¼‰
        
    def get_deployment_annotations(self, deployment_name, namespace='default'):
        """è·å–deploymentçš„æ³¨è§£é…ç½®"""
        try:
            deployment = self.v1.read_namespaced_deployment(
                name=deployment_name, 
                namespace=namespace
            )
            return deployment.metadata.annotations
        except Exception as e:
            logger.error(f"è·å–deploymentæ³¨è§£å¤±è´¥: {e}")
            return {}
    
    def get_current_replicas(self, deployment_name, namespace='default'):
        """è·å–å½“å‰å‰¯æœ¬æ•°"""
        try:
            deployment = self.v1.read_namespaced_deployment(
                name=deployment_name,
                namespace=namespace
            )
            return deployment.spec.replicas
        except Exception as e:
            logger.error(f"è·å–å‰¯æœ¬æ•°å¤±è´¥: {e}")
            # ä¸ä½¿ç”¨å‡æ•°æ®ï¼ŒæŠ›å‡ºå¼‚å¸¸è®©ä¸Šå±‚å¤„ç†
            raise Exception(f"æ— æ³•è·å–å‰¯æœ¬æ•°: {e}")
    
    def update_replicas(self, deployment_name, new_replicas, namespace='default'):
        """æ›´æ–°å‰¯æœ¬æ•°"""
        try:
            # æ›´æ–°deploymentçš„å‰¯æœ¬æ•°
            body = {'spec': {'replicas': new_replicas}}
            self.v1.patch_namespaced_deployment(
                name=deployment_name,
                namespace=namespace,
                body=body
            )
            logger.info(f"æˆåŠŸå°† {deployment_name} å‰¯æœ¬æ•°è°ƒæ•´ä¸º {new_replicas}")
            return True
        except Exception as e:
            logger.error(f"æ›´æ–°å‰¯æœ¬æ•°å¤±è´¥: {e}")
            return False
    
    async def get_app_pods(self, app_name: str, namespace: str = "default") -> List:
        """è·å–åº”ç”¨çš„æ‰€æœ‰pod"""
        try:
            # é€šè¿‡label selectoræŸ¥æ‰¾pods
            selectors = [
                f"app={app_name}",
                f"app.kubernetes.io/name={app_name}",
                f"k8s-app={app_name}"
            ]
            
            pods = []
            for selector in selectors:
                try:
                    pod_list = self.core_v1.list_namespaced_pod(
                        namespace=namespace,
                        label_selector=selector
                    )
                    if pod_list.items:
                        pods.extend(pod_list.items)
                        break
                except:
                    continue
            
            if not pods:
                # é€šè¿‡åç§°æ¨¡ç³ŠåŒ¹é…
                all_pods = self.core_v1.list_namespaced_pod(namespace=namespace)
                pods = [pod for pod in all_pods.items 
                       if app_name in pod.metadata.name]
            
            return pods
            
        except Exception as e:
            logger.error(f"è·å–åº”ç”¨ {app_name} çš„podså¤±è´¥: {e}")
            return []
    
    async def get_pod_network_metrics(self, pod_name: str, namespace: str = "default") -> Dict:
        """è·å–podçš„ç½‘ç»œæŒ‡æ ‡"""
        try:
            result = subprocess.run(
                ['kubectl', 'exec', '-n', namespace, pod_name, '--', 'cat', '/proc/net/dev'],
                capture_output=True, text=True, timeout=15
            )
            
            if result.returncode == 0:
                return self._parse_proc_net_dev(result.stdout)
            else:
                logger.warning(f"kubectl execå¤±è´¥: {result.stderr}")
                
        except subprocess.TimeoutExpired:
            logger.warning(f"kubectl execè¶…æ—¶: {pod_name}")
        except FileNotFoundError:
            logger.warning("kubectlå‘½ä»¤æœªæ‰¾åˆ°")
        except Exception as e:
            logger.warning(f"è·å–pod {pod_name} ç½‘ç»œæŒ‡æ ‡å¤±è´¥: {e}")
        
        return {'rx_bytes': 0, 'tx_bytes': 0}
    
    def _parse_proc_net_dev(self, content: str) -> Dict:
        """è§£æ/proc/net/devå†…å®¹"""
        try:
            lines = content.strip().split('\n')
            total_rx_bytes = 0
            total_tx_bytes = 0
            
            for i, line in enumerate(lines):
                if i < 2:  # è·³è¿‡å¤´éƒ¨ä¸¤è¡Œ
                    continue
                
                parts = line.split()
                if len(parts) >= 17 and ':' in parts[0]:
                    try:
                        rx_bytes = int(parts[1])
                        tx_bytes = int(parts[9])
                        
                        total_rx_bytes += rx_bytes
                        total_tx_bytes += tx_bytes
                        
                    except (ValueError, IndexError):
                        continue
            
            return {
                'rx_bytes': total_rx_bytes,
                'tx_bytes': total_tx_bytes
            }
            
        except Exception as e:
            logger.warning(f"è§£æç½‘ç»œæŒ‡æ ‡å¤±è´¥: {e}")
            return {'rx_bytes': 0, 'tx_bytes': 0}
    
    async def get_app_traffic(self, app_name: str, namespace: str = "default") -> Dict:
        """è·å–åº”ç”¨æµé‡æ•°æ®"""
        try:
            pods = await self.get_app_pods(app_name, namespace)
            if not pods:
                logger.warning(f"æœªæ‰¾åˆ°åº”ç”¨ {app_name} çš„pods")
                return {
                    'total_rx_bytes': 0,
                    'total_tx_bytes': 0,
                    'pod_count': 0
                }
            
            total_rx_bytes = 0
            total_tx_bytes = 0
            
            for pod in pods:
                if pod.status.phase != 'Running':
                    continue
                
                metrics = await self.get_pod_network_metrics(pod.metadata.name, namespace)
                total_rx_bytes += metrics['rx_bytes']
                total_tx_bytes += metrics['tx_bytes']
            
            return {
                'total_rx_bytes': total_rx_bytes,
                'total_tx_bytes': total_tx_bytes,
                'pod_count': len([p for p in pods if p.status.phase == 'Running'])
            }
            
        except Exception as e:
            logger.error(f"è·å–åº”ç”¨ {app_name} æµé‡æ•°æ®å¤±è´¥: {e}")
            return {
                'total_rx_bytes': 0,
                'total_tx_bytes': 0,
                'pod_count': 0
            }
    
    async def get_app_bandwidth_kbps(self, app_name: str, namespace: str = "default", sampling_interval: float = None):
        """
        è·å–åº”ç”¨çš„å®æ—¶å¸¦å®½ (Kbps) - ä½¿ç”¨å’Œapp_traffic_apiç›¸åŒçš„ç®—æ³•
        """
        if sampling_interval is None:
            sampling_interval = self.sampling_interval
        
        try:
            logger.info(f"å¼€å§‹è®¡ç®—åº”ç”¨ {app_name} çš„å®æ—¶å¸¦å®½ï¼Œé‡‡æ ·é—´éš”: {sampling_interval}ç§’")
            
            # ç¬¬ä¸€æ¬¡é‡‡æ ·
            first_sample = await self.get_app_traffic(app_name, namespace)
            first_time = datetime.now()
            
            # ç­‰å¾…é‡‡æ ·é—´éš”
            await asyncio.sleep(sampling_interval)
            
            # ç¬¬äºŒæ¬¡é‡‡æ ·
            second_sample = await self.get_app_traffic(app_name, namespace)
            second_time = datetime.now()
            
            # è®¡ç®—å®é™…æ—¶é—´é—´éš”
            actual_interval = (second_time - first_time).total_seconds()
            
            # è®¡ç®—å¸¦å®½ (bps)
            rx_bps = (second_sample['total_rx_bytes'] - first_sample['total_rx_bytes']) / actual_interval
            tx_bps = (second_sample['total_tx_bytes'] - first_sample['total_tx_bytes']) / actual_interval
            
            # è½¬æ¢ä¸º Kbps
            rx_kbps = max(0, rx_bps / 1024)
            tx_kbps = max(0, tx_bps / 1024)
            total_kbps = rx_kbps + tx_kbps
            
            logger.info(f"åº”ç”¨ {app_name} å¸¦å®½: RX={rx_kbps:.2f}Kbps, TX={tx_kbps:.2f}Kbps, æ€»è®¡={total_kbps:.2f}Kbps")
            
            return {
                'rx_kbps': rx_kbps,
                'tx_kbps': tx_kbps,
                'total_kbps': total_kbps,
                'pod_count': second_sample['pod_count'],
                'sampling_interval': actual_interval
            }
            
        except Exception as e:
            logger.error(f"è·å–åº”ç”¨ {app_name} å¸¦å®½å¤±è´¥: {e}")
            # å°è¯•è·å–çœŸå®çš„podæ•°é‡ï¼Œè€Œä¸æ˜¯ä½¿ç”¨å‡æ•°æ®
            try:
                pods = await self.get_app_pods(app_name, namespace)
                real_pod_count = len([p for p in pods if p.status.phase == 'Running'])
            except:
                real_pod_count = 0
            
            return {
                'rx_kbps': 0,
                'tx_kbps': 0,
                'total_kbps': 0,
                'pod_count': real_pod_count,  # ä½¿ç”¨çœŸå®Podæ•°é‡ï¼Œä¸æ˜¯å‡æ•°æ®
                'sampling_interval': sampling_interval
            }
    
    def calculate_min_threshold(self, max_threshold):
        """
        è®¡ç®—æœ€å°é˜ˆå€¼ï¼Œé¿å…é¢‘ç¹æ‰©ç¼©å®¹
        min = max * 0.3ï¼Œæä¾›è¶³å¤Ÿçš„ç¼“å†²åŒºé—´
        """
        return max_threshold * 0.3
    
    def should_scale(self, current_time):
        """æ£€æŸ¥æ˜¯å¦åº”è¯¥è¿›è¡Œæ‰©ç¼©å®¹ï¼ˆå†·å´æœŸæ£€æŸ¥ï¼‰"""
        if not self.scale_history:
            return True
        
        last_scale_time = self.scale_history[-1]['time']
        time_diff = (current_time - last_scale_time).total_seconds()
        
        return time_diff >= self.cooldown_period
    
    def _validate_annotations(self, annotations):
        """éªŒè¯annotationsä¸­æ˜¯å¦åŒ…å«æ‰€æœ‰å¿…è¦å­—æ®µ"""
        required_fields = [
            'deploy.cloud.sealos.io/network-hpa',
            'deploy.cloud.sealos.io/minReplicas',
            'deploy.cloud.sealos.io/maxReplicas'
        ]
        
        for field in required_fields:
            if field not in annotations:
                logger.error(f"ç¼ºå°‘å¿…è¦é…ç½®: {field}")
                return False
        return True
    
    async def make_scaling_decision(self, deployment_name, namespace='default'):
        """åšå‡ºæ‰©ç¼©å®¹å†³ç­– - åŸºäºçœŸå®å¸¦å®½æ•°æ®"""
        try:
            # è·å–é…ç½®
            annotations = self.get_deployment_annotations(deployment_name, namespace)
            
            if not annotations or 'deploy.cloud.sealos.io/network-hpa' not in annotations:
                logger.info("è‡ªåŠ¨æ‰©ç¼©å®¹æœªå¯ç”¨ (ç¼ºå°‘ deploy.cloud.sealos.io/network-hpa é…ç½®)")
                return
            
            # éªŒè¯æ‰€æœ‰å¿…è¦å­—æ®µ
            if not self._validate_annotations(annotations):
                return
            
            max_rate = float(annotations['deploy.cloud.sealos.io/network-hpa'])  # Kbps
            min_instances = int(annotations['deploy.cloud.sealos.io/minReplicas'])
            max_instances = int(annotations['deploy.cloud.sealos.io/maxReplicas'])
            
            # è®¡ç®—æœ€å°é˜ˆå€¼
            min_rate = self.calculate_min_threshold(max_rate)
            
            # è·å–å½“å‰çŠ¶æ€
            try:
                current_replicas = self.get_current_replicas(deployment_name, namespace)
            except Exception as e:
                logger.error(f"æ— æ³•è·å–å½“å‰å‰¯æœ¬æ•°ï¼Œè·³è¿‡æœ¬æ¬¡æ‰©ç¼©å®¹æ£€æŸ¥: {e}")
                return
            
            # è·å–çœŸå®å¸¦å®½æ•°æ® (å’Œapp_traffic_apiç›¸åŒçš„ç®—æ³•)
            bandwidth_data = await self.get_app_bandwidth_kbps(deployment_name, namespace)
            app_bandwidth_kbps = bandwidth_data['total_kbps']
            
            # è®¡ç®—æ¯ä¸ªå®ä¾‹çš„å¹³å‡å¸¦å®½è´Ÿè½½
            x = app_bandwidth_kbps / current_replicas if current_replicas > 0 else app_bandwidth_kbps
            
            logger.info(f"å½“å‰æŒ‡æ ‡ - å‰¯æœ¬æ•°: {current_replicas}, æ€»å¸¦å®½: {app_bandwidth_kbps:.2f}Kbps, "
                       f"æ¯å®ä¾‹è´Ÿè½½: {x:.2f}Kbps, é˜ˆå€¼èŒƒå›´: [{min_rate:.2f}, {max_rate}]")
            
            current_time = datetime.now()
            new_replicas = current_replicas
            action = "ä¿æŒ"
            
            # æ‰©ç¼©å®¹å†³ç­–
            if x > max_rate and current_replicas < max_instances:
                if self.should_scale(current_time):
                    new_replicas = min(current_replicas + 1, max_instances)
                    action = "æ‰©å®¹"
                else:
                    remaining = self.cooldown_period - (current_time - self.scale_history[-1]['time']).total_seconds()
                    logger.info(f"æ‰©å®¹è¯·æ±‚è¢«å†·å´æœŸé™åˆ¶ (å‰©ä½™ {remaining:.0f}s)")
                    
            elif x < min_rate and current_replicas > min_instances:
                if self.should_scale(current_time):
                    new_replicas = max(current_replicas - 1, min_instances)
                    action = "ç¼©å®¹"
                else:
                    remaining = self.cooldown_period - (current_time - self.scale_history[-1]['time']).total_seconds()
                    logger.info(f"ç¼©å®¹è¯·æ±‚è¢«å†·å´æœŸé™åˆ¶ (å‰©ä½™ {remaining:.0f}s)")
            
            # æ‰§è¡Œæ‰©ç¼©å®¹
            if new_replicas != current_replicas:
                if self.update_replicas(deployment_name, new_replicas, namespace):
                    # è®°å½•æ‰©ç¼©å®¹å†å²
                    self.scale_history.append({
                        'time': current_time,
                        'action': action,
                        'from': current_replicas,
                        'to': new_replicas,
                        'trigger_rate': x,
                        'total_bandwidth': app_bandwidth_kbps
                    })
                    
                    # åªä¿ç•™æœ€è¿‘10æ¬¡è®°å½•
                    if len(self.scale_history) > 10:
                        self.scale_history = self.scale_history[-10:]
                    
                    logger.info(f"âœ… {action}æˆåŠŸ: {current_replicas} -> {new_replicas} (è§¦å‘å€¼: {x:.2f}Kbps)")
            else:
                logger.info(f"ğŸ“Š {action}: å½“å‰å‰¯æœ¬æ•° {current_replicas} å·²æ»¡è¶³éœ€æ±‚")
                
        except Exception as e:
            logger.error(f"æ‰©ç¼©å®¹å†³ç­–è¿‡ç¨‹å‘ç”Ÿé”™è¯¯: {e}")
    
    def print_history(self):
        """æ‰“å°æ‰©ç¼©å®¹å†å²"""
        if not self.scale_history:
            logger.info("æš‚æ— æ‰©ç¼©å®¹å†å²è®°å½•")
            return
            
        logger.info("ğŸ“ˆ æ‰©ç¼©å®¹å†å²è®°å½•:")
        for record in self.scale_history[-5:]:  # æ˜¾ç¤ºæœ€è¿‘5æ¬¡
            logger.info(f"  {record['time'].strftime('%H:%M:%S')} - {record['action']}: "
                       f"{record['from']} -> {record['to']} (è´Ÿè½½: {record['trigger_rate']:.2f}Kbps)")
    
    async def get_all_autoscale_deployments(self, namespace='default'):
        """è·å–æ‰€æœ‰å¯ç”¨äº†å¸¦å®½è‡ªåŠ¨æ‰©ç¼©å®¹çš„deployment"""
        try:
            deployments = []
            
            # è·å–æŒ‡å®šnamespaceçš„æ‰€æœ‰deployment
            if namespace == 'all':
                # è·å–æ‰€æœ‰namespaceçš„deployment
                namespaces = self.core_v1.list_namespace()
                for ns in namespaces.items:
                    ns_name = ns.metadata.name
                    # è·³è¿‡ç³»ç»Ÿnamespace
                    if ns_name.startswith('kube-') or ns_name in ['kube-system', 'kube-public', 'kube-node-lease']:
                        continue
                    try:
                        dep_list = self.v1.list_namespaced_deployment(namespace=ns_name)
                        for dep in dep_list.items:
                            if dep.metadata.annotations and 'deploy.cloud.sealos.io/network-hpa' in dep.metadata.annotations:
                                deployments.append({
                                    'name': dep.metadata.name,
                                    'namespace': ns_name,
                                    'annotations': dep.metadata.annotations
                                })
                    except Exception as e:
                        logger.warning(f"è·å–namespace {ns_name} çš„deploymentå¤±è´¥: {e}")
            else:
                # è·å–æŒ‡å®šnamespaceçš„deployment
                dep_list = self.v1.list_namespaced_deployment(namespace=namespace)
                for dep in dep_list.items:
                    if dep.metadata.annotations and 'deploy.cloud.sealos.io/network-hpa' in dep.metadata.annotations:
                        deployments.append({
                            'name': dep.metadata.name,
                            'namespace': namespace,
                            'annotations': dep.metadata.annotations
                        })
            
            return deployments
            
        except Exception as e:
            logger.error(f"è·å–å¯ç”¨è‡ªåŠ¨æ‰©ç¼©å®¹çš„deploymentå¤±è´¥: {e}")
            return []

    async def run_single_app(self, deployment_name, namespace):
        """ä¸ºå•ä¸ªåº”ç”¨è¿è¡Œæ‰©ç¼©å®¹ç›‘æ§"""
        app_scale_history = []  # æ¯ä¸ªåº”ç”¨ç‹¬ç«‹çš„æ‰©ç¼©å®¹å†å²
        
        while True:
            try:
                logger.info(f"ğŸ” æ£€æŸ¥åº”ç”¨ {namespace}/{deployment_name}")
                
                # ä¸´æ—¶ä¿å­˜å½“å‰çš„scale_history
                original_history = self.scale_history
                self.scale_history = app_scale_history
                
                await self.make_scaling_decision(deployment_name, namespace)
                
                # ä¿å­˜åº”ç”¨çš„æ‰©ç¼©å®¹å†å²
                app_scale_history = self.scale_history
                # æ¢å¤åŸæ¥çš„history
                self.scale_history = original_history
                
                await asyncio.sleep(60)  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
                
            except Exception as e:
                logger.error(f"åº”ç”¨ {namespace}/{deployment_name} æ‰©ç¼©å®¹è¿‡ç¨‹å‘ç”Ÿé”™è¯¯: {e}")
                await asyncio.sleep(60)

    async def run(self, namespace='default'):
        """å¯åŠ¨é›†ç¾¤çº§åˆ«çš„åŸºäºå¸¦å®½è‡ªåŠ¨æ‰©ç¼©å®¹å™¨"""
        logger.info("ğŸš€ é›†ç¾¤çº§åˆ«å¸¦å®½è‡ªåŠ¨æ‰©ç¼©å®¹å™¨å¯åŠ¨")
        logger.info(f"ç›‘æ§èŒƒå›´: {namespace}")
        logger.info("æ£€æŸ¥é—´éš”: 60ç§’")
        logger.info("å†·å´æœŸ: 120ç§’")
        logger.info("è´Ÿè½½æŒ‡æ ‡: çœŸå®ç½‘ç»œå¸¦å®½ (Kbps)")
        logger.info("è§¦å‘æ¡ä»¶: deploymentåŒ…å« deploy.cloud.sealos.io/network-hpa æ³¨è§£")
        
        running_tasks = {}  # å­˜å‚¨æ¯ä¸ªåº”ç”¨çš„ç›‘æ§ä»»åŠ¡
        
        while True:
            try:
                logger.info(f"\n{'='*60}")
                logger.info(f"ğŸ” æ‰«æé›†ç¾¤ä¸­çš„è‡ªåŠ¨æ‰©ç¼©å®¹åº”ç”¨ - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
                
                # è·å–æ‰€æœ‰å¯ç”¨äº†è‡ªåŠ¨æ‰©ç¼©å®¹çš„deployment
                autoscale_deployments = await self.get_all_autoscale_deployments(namespace)
                
                logger.info(f"ğŸ“Š å‘ç° {len(autoscale_deployments)} ä¸ªå¯ç”¨è‡ªåŠ¨æ‰©ç¼©å®¹çš„åº”ç”¨:")
                for dep in autoscale_deployments:
                    hpa_value = dep['annotations'].get('deploy.cloud.sealos.io/network-hpa', 'N/A')
                    logger.info(f"   â€¢ {dep['namespace']}/{dep['name']} (é˜ˆå€¼: {hpa_value}Kbps)")
                
                # å¯åŠ¨æ–°å‘ç°çš„åº”ç”¨ç›‘æ§
                current_apps = set()
                for dep in autoscale_deployments:
                    app_key = f"{dep['namespace']}/{dep['name']}"
                    current_apps.add(app_key)
                    
                    if app_key not in running_tasks:
                        logger.info(f"ğŸš€ å¯åŠ¨æ–°åº”ç”¨ç›‘æ§: {app_key}")
                        task = asyncio.create_task(
                            self.run_single_app(dep['name'], dep['namespace'])
                        )
                        running_tasks[app_key] = task
                
                # åœæ­¢å·²åˆ é™¤åº”ç”¨çš„ç›‘æ§
                removed_apps = set(running_tasks.keys()) - current_apps
                for app_key in removed_apps:
                    logger.info(f"ğŸ›‘ åœæ­¢åº”ç”¨ç›‘æ§: {app_key}")
                    running_tasks[app_key].cancel()
                    del running_tasks[app_key]
                
                # æ˜¾ç¤ºå½“å‰ç›‘æ§çŠ¶æ€
                logger.info(f"ğŸ“ˆ å½“å‰ç›‘æ§ {len(running_tasks)} ä¸ªåº”ç”¨")
                self.print_cluster_status(running_tasks)
                
                logger.info("ğŸ’¤ ç­‰å¾…ä¸‹æ¬¡é›†ç¾¤æ‰«æ...")
                await asyncio.sleep(300)  # æ¯5åˆ†é’Ÿé‡æ–°æ‰«æé›†ç¾¤
                
            except KeyboardInterrupt:
                logger.info("ğŸ‘‹ æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œæ­£åœ¨é€€å‡º...")
                # å–æ¶ˆæ‰€æœ‰ä»»åŠ¡
                for task in running_tasks.values():
                    task.cancel()
                break
            except Exception as e:
                logger.error(f"é›†ç¾¤æ‰«æè¿‡ç¨‹å‘ç”Ÿé”™è¯¯: {e}")
                await asyncio.sleep(60)

    def print_cluster_status(self, running_tasks):
        """æ˜¾ç¤ºé›†ç¾¤ç›‘æ§çŠ¶æ€"""
        if not running_tasks:
            logger.info("   æš‚æ— åº”ç”¨åœ¨ç›‘æ§ä¸­")
            return
            
        logger.info("   ç›‘æ§ä¸­çš„åº”ç”¨:")
        for app_key in running_tasks.keys():
            logger.info(f"     âœ… {app_key}")
        
        # æ˜¾ç¤ºæœ€è¿‘çš„æ‰©ç¼©å®¹å†å²
        if self.scale_history:
            logger.info("ğŸ“ˆ æœ€è¿‘æ‰©ç¼©å®¹è®°å½•:")
            for record in self.scale_history[-3:]:  # æ˜¾ç¤ºæœ€è¿‘3æ¬¡
                logger.info(f"   {record['time'].strftime('%H:%M:%S')} - {record['action']}: "
                           f"{record['from']} -> {record['to']} (è´Ÿè½½: {record['trigger_rate']:.2f}Kbps)")

def run_autoscaler_all():
    """ä¾› app.py çº¿ç¨‹è°ƒç”¨çš„å‡½æ•°ï¼Œç›‘æ§æ‰€æœ‰namespace"""
    try:
        logger.info("ğŸš€ å¸¦å®½è‡ªåŠ¨æ‰©ç¼©å®¹å™¨ä½œä¸ºçº¿ç¨‹å¯åŠ¨")
        autoscaler = BandwidthAutoscaler()
        asyncio.run(autoscaler.run('all'))
    except Exception as e:
        logger.error(f"å¸¦å®½è‡ªåŠ¨æ‰©ç¼©å®¹å™¨çº¿ç¨‹å¯åŠ¨å¤±è´¥: {e}")

async def main():
    """ä¸»å‡½æ•°"""
    import sys
    
    print("ğŸ¯ K8s é›†ç¾¤çº§å¸¦å®½è‡ªåŠ¨æ‰©ç¼©å®¹å™¨")
    print("ä½œè€…: AI Assistant") 
    print("è¯´æ˜: åŸºäºçœŸå®ç½‘ç»œå¸¦å®½ (Kbps) çš„æ™ºèƒ½æ‰©ç¼©å®¹")
    print("èŒƒå›´: ç›‘æ§æ•´ä¸ªé›†ç¾¤ä¸­æ‰€æœ‰å¯ç”¨æ‰©ç¼©å®¹çš„åº”ç”¨")
    print("è§¦å‘: deploymentåŒ…å« deploy.cloud.sealos.io/network-hpa æ³¨è§£")
    print("-" * 60)
    
    # æ”¯æŒå‘½ä»¤è¡Œå‚æ•°æŒ‡å®šnamespace
    namespace = 'default'
    if len(sys.argv) > 1:
        namespace = sys.argv[1]
    
    print(f"ğŸ¯ ç›‘æ§èŒƒå›´: {namespace}")
    print("ğŸ’¡ ä½¿ç”¨æ–¹å¼:")
    print("   python bandwidth_autoscaler.py          # ç›‘æ§default namespace")
    print("   python bandwidth_autoscaler.py all      # ç›‘æ§æ‰€æœ‰namespace (é™¤ç³»ç»Ÿnamespace)")
    print("   python bandwidth_autoscaler.py ns-xxx   # ç›‘æ§æŒ‡å®šnamespace")
    print("-" * 60)
    
    try:
        autoscaler = BandwidthAutoscaler()
        
        # å¯åŠ¨é›†ç¾¤çº§æ‰©ç¼©å®¹å™¨
        await autoscaler.run(namespace)
    except KeyboardInterrupt:
        print("\nğŸ‘‹ ç¨‹åºé€€å‡º")
    except Exception as e:
        print(f"å¯åŠ¨å¤±è´¥: {e}")
        print("è¯·æ£€æŸ¥K8sé›†ç¾¤è¿æ¥")

if __name__ == "__main__":
    asyncio.run(main()) 