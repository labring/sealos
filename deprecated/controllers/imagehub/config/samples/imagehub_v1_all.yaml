# Copyright © 2023 sealos.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

apiVersion: imagehub.sealos.io/v1
kind: Image
metadata:
  name: labring.openebs.v1.9.0
spec:
  name: labring/openebs:v1.9.0
  detail:
    url: https://openebs.io/
    keywords: [ storage ]
    description: OpenEBS helps Developers and Platform SREs easily deploy Kubernetes Stateful Workloads that require fast and highly reliable container attached storage. OpenEBS turns any storage available on the Kubernetes worker nodes into local or distributed Kubernetes Persistent Volumes.
    icon: https://openebs.io/images/logos/logo.svg
    docs: |
      # OpenEBS

      [![Releases](https://img.shields.io/github/release/openebs/openebs/all.svg?style=flat-square)](https://github.com/openebs/openebs/releases)
      [![Slack channel #openebs](https://img.shields.io/badge/slack-openebs-brightgreen.svg?logo=slack)](https://kubernetes.slack.com/messages/openebs)
      [![Twitter](https://img.shields.io/twitter/follow/openebs.svg?style=social&label=Follow)](https://twitter.com/intent/follow?screen_name=openebs)
      [![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](https://github.com/openebs/openebs/blob/master/CONTRIBUTING.md)
      [![FOSSA Status](https://app.fossa.com/api/projects/git%2Bgithub.com%2Fopenebs%2Fopenebs.svg?type=shield)](https://app.fossa.com/projects/git%2Bgithub.com%2Fopenebs%2Fopenebs?ref=badge_shield)
      [![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/1754/badge)](https://bestpractices.coreinfrastructure.org/projects/1754)

      https://openebs.io/

      **Read this in**
      [🇩🇪](translations/README.de.md)	
      [🇷🇺](translations/README.ru.md)	
      [🇹🇷](translations/README.tr.md)	
      [🇺🇦](translations/README.ua.md)	
      [🇨🇳](translations/README.zh.md)	
      [🇫🇷](translations/README.fr.md)
      [🇧🇷](translations/README.pt-BR.md)
      [🇪🇸](translations/README.es.md)
      [🇵🇱](translations/README.pl.md)
      **[other languages](translations/#readme).**

      **OpenEBS** is the most widely deployed and easy to use open-source storage solution for Kubernetes.

      **OpenEBS** is the leading open-source example of a category of cloud native storage solutions sometimes called [Container Attached Storage](https://www.cncf.io/blog/2018/04/19/container-attached-storage-a-primer/). **OpenEBS** is listed as an open-source example in the [CNCF Storage White Paper](https://github.com/cncf/tag-storage/blob/master/CNCF%20Storage%20Whitepaper%20V2.pdf) under the hyperconverged storage solutions.

      Some key aspects that make OpenEBS different compared to other traditional storage solutions:
      - Built using the micro-services architecture like the applications it serves. OpenEBS is itself deployed as a set of containers on Kubernetes worker nodes. Uses Kubernetes itself to orchestrate and manage OpenEBS components.
      - Built completely in userspace making it highly portable to run across any OS/platform.
      - Completely intent-driven, inheriting the same principles that drive the ease of use with Kubernetes.
      - OpenEBS supports a range of storage engines so that developers can deploy the storage technology appropriate to their application design objectives. Distributed applications like Cassandra can use the LocalPV engine for lowest latency writes. Monolithic applications like MySQL and PostgreSQL can use the ZFS engine (cStor) for resilience. Streaming applications like Kafka can use the NVMe engine [Mayastor](https://github.com/openebs/Mayastor) for best performance in edge environments. Across engine types, OpenEBS provides a consistent framework for high availability, snapshots, clones and manageability.

      OpenEBS itself is deployed as just another container on your host and enables storage services that can be designated on a per pod, application, cluster or container level, including:
      - Automate the management of storage attached to the Kubernetes worker nodes and allow the storage to be used for Dynamically provisioning OpenEBS Replicated or Local PVs.
      - Data persistence across nodes, dramatically reducing time spent rebuilding Cassandra rings for example.
      - Synchronous replication of volume data across availability zones improving availability and decreasing attach/detach times for example.
      - A common layer so whether you are running on AKS, or your bare metal, or GKE, or AWS - your wiring and developer experience for storage services is as similar as possible.
      - Backup and Restore of volume data to and from S3 and other targets.

      An added advantage of being a completely Kubernetes native solution is that administrators and developers can interact and manage OpenEBS using all the wonderful tooling that is available for Kubernetes like kubectl, Helm, Prometheus, Grafana, Weave Scope, etc.

      **Our vision** is simple: let storage and storage services for persistent workloads be fully integrated into the environment so that each team and workload benefits from the granularity of control and Kubernetes native behaviour.

      ## Scalability

      OpenEBS can scale to include an arbitrarily large number of containerized storage controllers. Kubernetes is used to provide fundamental pieces such as using etcd for inventory. OpenEBS scales to the extent your Kubernetes scales.

      ## Installation and Getting Started

      OpenEBS can be set up in a few easy steps. You can get going on your choice of Kubernetes cluster by having open-iscsi installed on the Kubernetes nodes and running the openebs-operator using kubectl.

      **Start the OpenEBS Services using operator**
      ```bash
      # apply this yaml
      kubectl apply -f https://openebs.github.io/charts/openebs-operator.yaml
      ```

      **Start the OpenEBS Services using helm**
      ```bash
      helm repo update
      helm install --namespace openebs --name openebs stable/openebs
      ```

      You could also follow our [QuickStart Guide](https://openebs.io/docs).

      OpenEBS can be deployed on any Kubernetes cluster - either in the cloud, on-premise or developer laptop (minikube). Note that there are no changes to the underlying kernel that are required as OpenEBS operates in userspace.  Please follow our [OpenEBS Setup](https://openebs.io/docs/user-guides/quickstart) documentation.

      ## Status

      OpenEBS is one of the most widely used and tested Kubernetes storage infrastructures in the industry. A CNCF Sandbox project since May 2019, OpenEBS is the first and only storage system to provide a consistent set of software-defined storage capabilities on multiple backends (local, nfs, zfs, nvme) across both on-premise and cloud systems, and was the first to open source its own Chaos Engineering Framework for Stateful Workloads, the [Litmus Project](https://litmuschaos.io), which the community relies on to automatically readiness assess the monthly cadence of OpenEBS versions. Enterprise customers have been using OpenEBS in production since 2018. 

      The status of various storage engines that power the OpenEBS Persistent Volumes are provided below. The key difference between the statuses are summarized below:
      - **alpha:** The API may change in incompatible ways in a later software release without notice, recommended for use only in short-lived testing clusters, due to increased risk of bugs and lack of long-term support.
      - **beta**: Support for the overall features will not be dropped, though details may change. Support for upgrading or migrating between versions will be provided, either through automation or manual steps.
      - **stable**: Features will appear in released software for many subsequent versions and support for upgrading between versions will be provided with software automation in the vast majority of scenarios.


      | Storage Engine | Status | Details |
      |---|---|---|
      | Jiva | stable | Best suited for running Replicated Block Storage on nodes that make use of ephemeral storage on the Kubernetes worker nodes |
      | cStor | stable | A preferred option for running on nodes that have Block Devices. Recommended option if Snapshot and Clones are required |
      | Local Volumes | stable | Best suited for Distributed Application that need low latency storage - direct-attached storage from the Kubernetes nodes. |
      | Mayastor | stable | Persistent storage solution for Kubernetes, with near-native NVMe performance and advanced data services. |

      For more details, please refer to [OpenEBS Documentation](https://openebs.io/docs/).

      ## Contributing

      OpenEBS welcomes your feedback and contributions in any form possible.

      - [Join OpenEBS community on Kubernetes Slack](https://kubernetes.slack.com)
        - Already signed up? Head to our discussions at [#openebs](https://kubernetes.slack.com/messages/openebs/)
      - Want to raise an issue or help with fixes and features?
        - See [open issues](https://github.com/openebs/openebs/issues)
        - See [contributing guide](./CONTRIBUTING.md)
        - Want to join our contributor community meetings, [check this out](./community/README.md).
      - Join our OpenEBS CNCF Mailing lists
        - For OpenEBS project updates, subscribe to [OpenEBS Announcements](https://lists.cncf.io/g/cncf-openebs-announcements)
        - For interacting with other OpenEBS users, subscribe to [OpenEBS Users](https://lists.cncf.io/g/cncf-openebs-users)

      ## Show me the Code

      This is a meta-repository for OpenEBS. Please start with the pinned repositories or with [OpenEBS Architecture](./contribute/design/README.md) document. 

      ## License

      OpenEBS is developed under [Apache License 2.0](https://github.com/openebs/openebs/blob/master/LICENSE) license at the project level. Some components of the project are derived from other open source projects and are distributed under their respective licenses.

      OpenEBS is part of the CNCF Projects.

      [![CNCF Sandbox Project](https://raw.githubusercontent.com/cncf/artwork/master/other/cncf-sandbox/horizontal/color/cncf-sandbox-horizontal-color.png)](https://landscape.cncf.io/selected=open-ebs)

      ## Commercial Offerings

      This is a list of third-party companies and individuals who provide products or services related to OpenEBS. OpenEBS is a CNCF project which does not endorse any company. The list is provided in alphabetical order.
      - [Clouds Sky GmbH](https://cloudssky.com/en/)
      - [CodeWave](https://codewave.eu/)
      - [DataCore](https://www.datacore.com/support/openebs/)
      - [Gridworkz Cloud Services](https://gridworkz.com/)
    ID: Unkonw
    arch: Unkonw
---
apiVersion: imagehub.sealos.io/v1
kind: Image
metadata:
  name: labring.helm.v3.8.2
spec:
  name: labring/helm:v3.8.2
  detail:
    url: https://helm.sh/
    keywords: [ Storage ]
    description: Helm is the best way to find, share, and use software built for Kubernetes
    icon: https://helm.sh/img/helm.svg
    docs: |
      # Helm

      [![CircleCI](https://circleci.com/gh/helm/helm.svg?style=shield)](https://circleci.com/gh/helm/helm)
      [![Go Report Card](https://goreportcard.com/badge/github.com/helm/helm)](https://goreportcard.com/report/github.com/helm/helm)
      [![GoDoc](https://img.shields.io/static/v1?label=godoc&message=reference&color=blue)](https://pkg.go.dev/helm.sh/helm/v3)
      [![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/3131/badge)](https://bestpractices.coreinfrastructure.org/projects/3131)

      Helm is a tool for managing Charts. Charts are packages of pre-configured Kubernetes resources.

      Use Helm to:

      - Find and use [popular software packaged as Helm Charts](https://artifacthub.io/packages/search?kind=0) to run in Kubernetes
      - Share your own applications as Helm Charts
      - Create reproducible builds of your Kubernetes applications
      - Intelligently manage your Kubernetes manifest files
      - Manage releases of Helm packages

      ## Helm in a Handbasket

      Helm is a tool that streamlines installing and managing Kubernetes applications.
      Think of it like apt/yum/homebrew for Kubernetes.

      - Helm renders your templates and communicates with the Kubernetes API
      - Helm runs on your laptop, CI/CD, or wherever you want it to run.
      - Charts are Helm packages that contain at least two things:
        - A description of the package (`Chart.yaml`)
        - One or more templates, which contain Kubernetes manifest files
      - Charts can be stored on disk, or fetched from remote chart repositories
        (like Debian or RedHat packages)

      ## Install


      Binary downloads of the Helm client can be found on [the Releases page](https://github.com/helm/helm/releases/latest).

      Unpack the `helm` binary and add it to your PATH and you are good to go!

      If you want to use a package manager:

      - [Homebrew](https://brew.sh/) users can use `brew install helm`.
      - [Chocolatey](https://chocolatey.org/) users can use `choco install kubernetes-helm`.
      - [Scoop](https://scoop.sh/) users can use `scoop install helm`.
      - [GoFish](https://gofi.sh/) users can use `gofish install helm`.
      - [Snapcraft](https://snapcraft.io/) users can use `snap install helm --classic`

      To rapidly get Helm up and running, start with the [Quick Start Guide](https://helm.sh/docs/intro/quickstart/).

      See the [installation guide](https://helm.sh/docs/intro/install/) for more options,
      including installing pre-releases.

      ## Docs

      Get started with the [Quick Start guide](https://helm.sh/docs/intro/quickstart/) or plunge into the [complete documentation](https://helm.sh/docs)

      ## Roadmap

      The [Helm roadmap uses GitHub milestones](https://github.com/helm/helm/milestones) to track the progress of the project.

      ## Community, discussion, contribution, and support

      You can reach the Helm community and developers via the following channels:

      - [Kubernetes Slack](https://kubernetes.slack.com):
        - [#helm-users](https://kubernetes.slack.com/messages/helm-users)
        - [#helm-dev](https://kubernetes.slack.com/messages/helm-dev)
        - [#charts](https://kubernetes.slack.com/messages/charts)
      - Mailing List:
        - [Helm Mailing List](https://lists.cncf.io/g/cncf-helm)
      - Developer Call: Thursdays at 9:30-10:00 Pacific ([meeting details](https://github.com/helm/community/blob/master/communication.md#meetings))

      ### Code of conduct

      Participation in the Helm community is governed by the [Code of Conduct](code-of-conduct.md).
    ID: Unkonw
    arch: Unkonw
---
apiVersion: imagehub.sealos.io/v1
kind: Image
metadata:
  name: labring.cert-manager.v1.8.0
spec:
  name: labring/cert-manager:v1.8.0
  detail:
    url: https://cert-manager.io/
    keywords: [ storage ]
    description: Cloud native certificate management. X.509 certificate management for Kubernetes and OpenShift
    icon: https://cert-manager.io/images/cert-manager-logo-icon.svg
    docs: |
      
      <p align="center">
        <img src="./logo/logo-small.png" height="256" width="256" alt="cert-manager project logo" />
      </p>
      <!-- note that the cert-manager logo in this repo is referred to in other README files in the cert-manager org
           as well as in Helm charts, etc.
           if you change its location or name, you'll need to update several other repos too! -->

      <p align="center"><a href="https://prow.build-infra.jetstack.net/?job=ci-cert-manager-bazel">
      <!-- prow build badge, godoc, and go report card-->
      <img alt="Build Status" src="https://prow.build-infra.jetstack.net/badge.svg?jobs=ci-cert-manager-bazel">
      </a>
      <a href="https://godoc.org/github.com/cert-manager/cert-manager"><img src="https://godoc.org/github.com/cert-manager/cert-manager?status.svg"></a>
      <a href="https://goreportcard.com/report/github.com/cert-manager/cert-manager"><img alt="Go Report Card" src="https://goreportcard.com/badge/github.com/cert-manager/cert-manager" /></a>
      <br />
      <a href="https://artifacthub.io/packages/search?repo=cert-manager"><img alt="Artifact Hub" src="https://img.shields.io/endpoint?URL=https://artifacthub.io/badge/repository/cert-manager" /></a>
      <a href="https://api.securityscorecards.dev/projects/github.com/cert-manager/cert-manager"><img src="https://api.securityscorecards.dev/projects/github.com/cert-manager/cert-manager/badge" alt="Scorecard score"/></a>
      </p>

      # cert-manager

      cert-manager adds certificates and certificate issuers as resource types in Kubernetes clusters, and simplifies the process of obtaining, renewing and using those certificates.

      It supports issuing certificates from a variety of sources, including Let's Encrypt (ACME), HashiCorp Vault, and Venafi TPP / TLS Protect Cloud, as well as local in-cluster issuance.

      cert-manager also ensures certificates remain valid and up to date, attempting to renew certificates at an appropriate time before expiry to reduce the risk of outages and remove toil.

      ![cert-manager high level overview diagram](https://cert-manager.io/images/high-level-overview.svg)

      ## Documentation

      Documentation for cert-manager can be found at [cert-manager.io](https://cert-manager.io/docs/).

      For the common use-case of automatically issuing TLS certificates for
      Ingress resources, see the [cert-manager nginx-ingress quick start guide](https://cert-manager.io/docs/tutorials/acme/nginx-ingress/).

      For a more comprensive guide to issuing your first certificate, see our [getting started guide](https://cert-manager.io/docs/getting-started/).

      ### Installation

      [Installation](https://cert-manager.io/docs/installation/) is documented on the website, with a variety of supported methods.

      ## Troubleshooting

      If you encounter any issues whilst using cert-manager, we have a number of ways to get help:

      - A [troubleshooting guide](https://cert-manager.io/docs/faq/troubleshooting/) on our website.
      - Our official [Kubernetes Slack channel](https://cert-manager.io/docs/contributing/#slack) - the quickest way to ask!
      - [Searching for an existing issue](https://github.com/cert-manager/cert-manager/issues).

      If you believe you've found a bug and cannot find an existing issue, feel free to [open a new issue](https://github.com/cert-manager/cert-manager/issues)!
      Be sure to include as much information as you can about your environment.

      ## Community

      The `cert-manager-dev` Google Group is used for project wide announcements and development coordination.
      Anybody can join the group by visiting [here](https://groups.google.com/forum/#!forum/cert-manager-dev)
      and clicking "Join Group". A Google account is required to join the group.

      ### Meetings

      We have several public meetings which any member of our Google Group is more than welcome to join!

      Check out the details on [our website](https://cert-manager.io/docs/contributing/#meetings). Feel
      free to drop in and ask questions, chat with us or just to say hi!

      ## Contributing

      We welcome pull requests with open arms! There's a lot of work to do here, and
      we're especially concerned with ensuring the longevity and reliability of the
      project. The [contributing guide](https://cert-manager.io/docs/contributing/)
      will help you get started.

      ## Coding Conventions

      Code style guidelines are documented on the [coding conventions](https://cert-manager.io/docs/contributing/coding-conventions/) page
      of the cert-manager website. Please try to follow those guidelines if you're submitting a pull request for cert-manager.

      ## Importing cert-manager as a Module

      ⚠️ Please note that cert-manager **does not** currently provide a Go module compatibility guarantee. That means that
      **most code under `pkg/` is subject to change in a breaking way**, even between minor or patch releases and even if
      the code is currently publicly exported.

      The lack of a Go module compatibility guarantee does not affect API version guarantees
      under the [Kubernetes Deprecation Policy](https://kubernetes.io/docs/reference/using-api/deprecation-policy/).

      For more details see [Importing cert-manager in Go](https://cert-manager.io/docs/contributing/importing/) on the
      cert-manager website.

      The import path for cert-manager versions 1.8 and later is `github.com/cert-manager/cert-manager`.

      For all versions of cert-manager before 1.8, including minor and patch releases, the import path is `github.com/jetstack/cert-manager`.

      ## Security Reporting

      Security is the number one priority for cert-manager. If you think you've found a security vulnerability, we'd love to hear from you.

      Follow the instructions in [SECURITY.md](./SECURITY.md) to make a report.

      ## Changelog

      [Every release](https://github.com/cert-manager/cert-manager/releases) on GitHub has a changelog,
      and we also publish release notes on [the website](https://cert-manager.io/docs/release-notes/).

      ## History

      cert-manager is loosely based upon the work of [kube-lego](https://github.com/jetstack/kube-lego)
      and has borrowed some wisdom from other similar projects such as [kube-cert-manager](https://github.com/PalmStoneGames/kube-cert-manager).


      <sub><sup>Logo design by [Zoe Paterson](https://zoepatersonmedia.com)</sup></sub>
    ID: Unkonw
    arch: Unkonw
---
apiVersion: imagehub.sealos.io/v1
kind: Image
metadata:
  name: labring.rook.v1.9.8
spec:
  name: labring/rook:v1.9.8
  detail:
    url: https://rook.io/
    keywords: [ Storage , operator ]
    description: |
      Storage Operators for Kubernetes
      Rook turns distributed storage systems into self-managing, self-scaling, self-healing storage services. It automates the tasks of a storage administrator: deployment, bootstrapping, configuration, provisioning, scaling, upgrading, migration, disaster recovery, monitoring, and resource management.
      Rook uses the power of the Kubernetes platform to deliver its services via a Kubernetes Operator for Ceph.
    icon: https://rook.io/images/rook-logo.svg
    docs: |
      <img alt="Rook" src="Documentation/media/logo.svg" width="50%" height="50%">

      [![CNCF Status](https://img.shields.io/badge/cncf%20status-graduated-blue.svg)](https://www.cncf.io/projects)
      [![GitHub release](https://img.shields.io/github/release/rook/rook/all.svg)](https://github.com/rook/rook/releases)
      [![Docker Pulls](https://img.shields.io/docker/pulls/rook/ceph)](https://hub.docker.com/u/rook)
      [![Go Report Card](https://goreportcard.com/badge/github.com/rook/rook)](https://goreportcard.com/report/github.com/rook/rook)
      [![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/1599/badge)](https://bestpractices.coreinfrastructure.org/projects/1599)
      [![Security scanning](https://github.com/rook/rook/actions/workflows/synk.yaml/badge.svg)](https://github.com/rook/rook/actions/workflows/synk.yaml)
      [![Slack](https://slack.rook.io/badge.svg)](https://slack.rook.io)
      [![Twitter Follow](https://img.shields.io/twitter/follow/rook_io.svg?style=social&label=Follow)](https://twitter.com/intent/follow?screen_name=rook_io&user_id=788180534543339520)

      # What is Rook?

      Rook is an open source **cloud-native storage orchestrator** for Kubernetes, providing the platform, framework, and support for a diverse set of storage solutions to natively integrate with cloud-native environments.

      Rook turns storage software into self-managing, self-scaling, and self-healing storage services. It does this by automating deployment, bootstrapping, configuration, provisioning, scaling, upgrading, migration, disaster recovery, monitoring, and resource management. Rook uses the facilities provided by the underlying cloud-native container management, scheduling and orchestration platform to perform its duties.

      Rook integrates deeply into cloud native environments leveraging extension points and providing a seamless experience for scheduling, lifecycle management, resource management, security, monitoring, and user experience.

      For more details about the storage solutions currently supported by Rook, please refer to the [project status section](#project-status) below.
      We plan to continue adding support for other storage systems and environments based on community demand and engagement in future releases. See our [roadmap](ROADMAP.md) for more details.

      Rook is hosted by the [Cloud Native Computing Foundation](https://cncf.io) (CNCF) as a [graduated](https://www.cncf.io/announcements/2020/10/07/cloud-native-computing-foundation-announces-rook-graduation/) level project. If you are a company that wants to help shape the evolution of technologies that are container-packaged, dynamically-scheduled and microservices-oriented, consider joining the CNCF. For details about who's involved and how Rook plays a role, read the CNCF [announcement](https://www.cncf.io/blog/2018/01/29/cncf-host-rook-project-cloud-native-storage-capabilities).

      ## Getting Started and Documentation

      For installation, deployment, and administration, see our [Documentation](https://rook.github.io/docs/rook/latest).

      ## Contributing

      We welcome contributions. See [Contributing](CONTRIBUTING.md) to get started.

      ## Report a Bug

      For filing bugs, suggesting improvements, or requesting new features, please open an [issue](https://github.com/rook/rook/issues).

      ### Reporting Security Vulnerabilities

      If you find a vulnerability or a potential vulnerability in Rook please let us know immediately at
      [cncf-rook-security@lists.cncf.io](mailto:cncf-rook-security@lists.cncf.io). We'll send a confirmation email to acknowledge your
      report, and we'll send an additional email when we've identified the issues positively or
      negatively.

      For further details, please see the complete [security release process](SECURITY.md).

      ## Contact

      Please use the following to reach members of the community:

      - Slack: Join our [slack channel](https://slack.rook.io)
      - GitHub: Start a [discussion](https://github.com/rook/rook/discussions) or open an [issue](https://github.com/rook/rook/issues)
      - Twitter: [@rook_io](https://twitter.com/rook_io)
      - Security topics: [cncf-rook-security@lists.cncf.io](#reporting-security-vulnerabilities)

      ### Community Meeting

      A regular community meeting takes place every other [Tuesday at 9:00 AM PT (Pacific Time)](https://zoom.us/j/392602367?pwd=NU1laFZhTWF4MFd6cnRoYzVwbUlSUT09).
      Convert to your [local timezone](http://www.thetimezoneconverter.com/?t=9:00&tz=PT%20%28Pacific%20Time%29).

      Any changes to the meeting schedule will be added to the [agenda doc](https://docs.google.com/document/d/1exd8_IG6DkdvyA0eiTtL2z5K2Ra-y68VByUUgwP7I9A/edit?usp=sharing) and posted to [Slack #announcements](https://rook-io.slack.com/messages/C76LLCEE7/).

      Anyone who wants to discuss the direction of the project, design and implementation reviews, or general questions with the broader community is welcome and encouraged to join.

      - Meeting link: <https://zoom.us/j/392602367?pwd=NU1laFZhTWF4MFd6cnRoYzVwbUlSUT09>
      - [Current agenda and past meeting notes](https://docs.google.com/document/d/1exd8_IG6DkdvyA0eiTtL2z5K2Ra-y68VByUUgwP7I9A/edit?usp=sharing)
      - [Past meeting recordings](https://www.youtube.com/playlist?list=PLP0uDo-ZFnQP6NAgJWAtR9jaRcgqyQKVy)

      ## Project Status

      The status of each storage provider supported by Rook can be found in the table below.
      Each API group is assigned its own individual status to reflect their varying maturity and stability.
      More details about API versioning and status in Kubernetes can be found on the Kubernetes [API versioning page](https://kubernetes.io/docs/concepts/overview/kubernetes-api/#api-versioning), but the key difference between the statuses are summarized below:

      - **Alpha:** The API may change in incompatible ways in a later software release without notice, recommended for use only in short-lived testing clusters, due to increased risk of bugs and lack of long-term support.
      - **Beta:** Support for the overall features will not be dropped, though details may change. Support for upgrading or migrating between versions will be provided, either through automation or manual steps.
      - **Stable:** Features will appear in released software for many subsequent versions and support for upgrading between versions will be provided with software automation in the vast majority of scenarios.

      | Name | Details                                                                                                                                                    | API Group       | Status |
      | ---- | ---------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------- | ------ |
      | Ceph | [Ceph](https://ceph.com/) is a distributed storage system that provides file, block and object storage and is deployed in large scale production clusters. | ceph.rook.io/v1 | Stable |

      This repo is for the Ceph storage provider. The [Cassandra](https://github.com/rook/cassandra) and [NFS](https://github.com/rook/nfs) storage providers moved to a separate repo to allow for each [storage provider](https://rook.github.io/docs/rook/latest/storage-providers.html) to have an independent development and release schedule.

      ### Official Releases

      Official releases of Rook can be found on the [releases page](https://github.com/rook/rook/releases).
      Please note that it is **strongly recommended** that you use [official releases](https://github.com/rook/rook/releases) of Rook, as unreleased versions from the master branch are subject to changes and incompatibilities that will not be supported in the official releases.
      Builds from the master branch can have functionality changed and even removed at any time without compatibility support and without prior notice.

      ## Licensing

      Rook is under the Apache 2.0 license.

      [![FOSSA Status](https://app.fossa.io/api/projects/git%2Bgithub.com%2Frook%2Frook.svg?type=large)](https://app.fossa.io/projects/git%2Bgithub.com%2Frook%2Frook?ref=badge_large)
    ID: Unkonw
    arch: Unkonw
---
apiVersion: imagehub.sealos.io/v1
kind: Image
metadata:
  name: labring.mysql-operator.8.0.23-14.1
spec:
  name: labring/mysql-operator:8.0.23-14.1
  detail:
    url: https://dev.mysql.com/doc/mysql-operator/en/
    keywords: [ Storage , Operator ]
    description: |
      MySQL Operator for Kubernetes manages MySQL InnoDB Cluster setups inside a Kubernetes Cluster. MySQL Operator for Kubernetes manages the full lifecycle with setup and maintenance including automating upgrades and backups.
    icon: https://rook.io/images/rook-logo.svg
    docs: |
      <img alt="Rook" src="Documentation/media/logo.svg" width="50%" height="50%">

      [![CNCF Status](https://img.shields.io/badge/cncf%20status-graduated-blue.svg)](https://www.cncf.io/projects)
      [![GitHub release](https://img.shields.io/github/release/rook/rook/all.svg)](https://github.com/rook/rook/releases)
      [![Docker Pulls](https://img.shields.io/docker/pulls/rook/ceph)](https://hub.docker.com/u/rook)
      [![Go Report Card](https://goreportcard.com/badge/github.com/rook/rook)](https://goreportcard.com/report/github.com/rook/rook)
      [![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/1599/badge)](https://bestpractices.coreinfrastructure.org/projects/1599)
      [![Security scanning](https://github.com/rook/rook/actions/workflows/synk.yaml/badge.svg)](https://github.com/rook/rook/actions/workflows/synk.yaml)
      [![Slack](https://slack.rook.io/badge.svg)](https://slack.rook.io)
      [![Twitter Follow](https://img.shields.io/twitter/follow/rook_io.svg?style=social&label=Follow)](https://twitter.com/intent/follow?screen_name=rook_io&user_id=788180534543339520)

      # What is Rook?

      Rook is an open source **cloud-native storage orchestrator** for Kubernetes, providing the platform, framework, and support for a diverse set of storage solutions to natively integrate with cloud-native environments.

      Rook turns storage software into self-managing, self-scaling, and self-healing storage services. It does this by automating deployment, bootstrapping, configuration, provisioning, scaling, upgrading, migration, disaster recovery, monitoring, and resource management. Rook uses the facilities provided by the underlying cloud-native container management, scheduling and orchestration platform to perform its duties.

      Rook integrates deeply into cloud native environments leveraging extension points and providing a seamless experience for scheduling, lifecycle management, resource management, security, monitoring, and user experience.

      For more details about the storage solutions currently supported by Rook, please refer to the [project status section](#project-status) below.
      We plan to continue adding support for other storage systems and environments based on community demand and engagement in future releases. See our [roadmap](ROADMAP.md) for more details.

      Rook is hosted by the [Cloud Native Computing Foundation](https://cncf.io) (CNCF) as a [graduated](https://www.cncf.io/announcements/2020/10/07/cloud-native-computing-foundation-announces-rook-graduation/) level project. If you are a company that wants to help shape the evolution of technologies that are container-packaged, dynamically-scheduled and microservices-oriented, consider joining the CNCF. For details about who's involved and how Rook plays a role, read the CNCF [announcement](https://www.cncf.io/blog/2018/01/29/cncf-host-rook-project-cloud-native-storage-capabilities).

      ## Getting Started and Documentation

      For installation, deployment, and administration, see our [Documentation](https://rook.github.io/docs/rook/latest).

      ## Contributing

      We welcome contributions. See [Contributing](CONTRIBUTING.md) to get started.

      ## Report a Bug

      For filing bugs, suggesting improvements, or requesting new features, please open an [issue](https://github.com/rook/rook/issues).

      ### Reporting Security Vulnerabilities

      If you find a vulnerability or a potential vulnerability in Rook please let us know immediately at
      [cncf-rook-security@lists.cncf.io](mailto:cncf-rook-security@lists.cncf.io). We'll send a confirmation email to acknowledge your
      report, and we'll send an additional email when we've identified the issues positively or
      negatively.

      For further details, please see the complete [security release process](SECURITY.md).

      ## Contact

      Please use the following to reach members of the community:

      - Slack: Join our [slack channel](https://slack.rook.io)
      - GitHub: Start a [discussion](https://github.com/rook/rook/discussions) or open an [issue](https://github.com/rook/rook/issues)
      - Twitter: [@rook_io](https://twitter.com/rook_io)
      - Security topics: [cncf-rook-security@lists.cncf.io](#reporting-security-vulnerabilities)

      ### Community Meeting

      A regular community meeting takes place every other [Tuesday at 9:00 AM PT (Pacific Time)](https://zoom.us/j/392602367?pwd=NU1laFZhTWF4MFd6cnRoYzVwbUlSUT09).
      Convert to your [local timezone](http://www.thetimezoneconverter.com/?t=9:00&tz=PT%20%28Pacific%20Time%29).

      Any changes to the meeting schedule will be added to the [agenda doc](https://docs.google.com/document/d/1exd8_IG6DkdvyA0eiTtL2z5K2Ra-y68VByUUgwP7I9A/edit?usp=sharing) and posted to [Slack #announcements](https://rook-io.slack.com/messages/C76LLCEE7/).

      Anyone who wants to discuss the direction of the project, design and implementation reviews, or general questions with the broader community is welcome and encouraged to join.

      - Meeting link: <https://zoom.us/j/392602367?pwd=NU1laFZhTWF4MFd6cnRoYzVwbUlSUT09>
      - [Current agenda and past meeting notes](https://docs.google.com/document/d/1exd8_IG6DkdvyA0eiTtL2z5K2Ra-y68VByUUgwP7I9A/edit?usp=sharing)
      - [Past meeting recordings](https://www.youtube.com/playlist?list=PLP0uDo-ZFnQP6NAgJWAtR9jaRcgqyQKVy)

      ## Project Status

      The status of each storage provider supported by Rook can be found in the table below.
      Each API group is assigned its own individual status to reflect their varying maturity and stability.
      More details about API versioning and status in Kubernetes can be found on the Kubernetes [API versioning page](https://kubernetes.io/docs/concepts/overview/kubernetes-api/#api-versioning), but the key difference between the statuses are summarized below:

      - **Alpha:** The API may change in incompatible ways in a later software release without notice, recommended for use only in short-lived testing clusters, due to increased risk of bugs and lack of long-term support.
      - **Beta:** Support for the overall features will not be dropped, though details may change. Support for upgrading or migrating between versions will be provided, either through automation or manual steps.
      - **Stable:** Features will appear in released software for many subsequent versions and support for upgrading between versions will be provided with software automation in the vast majority of scenarios.

      | Name | Details                                                                                                                                                    | API Group       | Status |
      | ---- | ---------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------- | ------ |
      | Ceph | [Ceph](https://ceph.com/) is a distributed storage system that provides file, block and object storage and is deployed in large scale production clusters. | ceph.rook.io/v1 | Stable |

      This repo is for the Ceph storage provider. The [Cassandra](https://github.com/rook/cassandra) and [NFS](https://github.com/rook/nfs) storage providers moved to a separate repo to allow for each [storage provider](https://rook.github.io/docs/rook/latest/storage-providers.html) to have an independent development and release schedule.

      ### Official Releases

      Official releases of Rook can be found on the [releases page](https://github.com/rook/rook/releases).
      Please note that it is **strongly recommended** that you use [official releases](https://github.com/rook/rook/releases) of Rook, as unreleased versions from the master branch are subject to changes and incompatibilities that will not be supported in the official releases.
      Builds from the master branch can have functionality changed and even removed at any time without compatibility support and without prior notice.

      ## Licensing

      Rook is under the Apache 2.0 license.

      [![FOSSA Status](https://app.fossa.io/api/projects/git%2Bgithub.com%2Frook%2Frook.svg?type=large)](https://app.fossa.io/projects/git%2Bgithub.com%2Frook%2Frook?ref=badge_large)
    ID: Unkonw
    arch: Unkonw
---
apiVersion: imagehub.sealos.io/v1
kind: Image
metadata:
  name: labring.clickhouse.0.18.4
spec:
  name: labring/clickhouse:0.18.4
  detail:
    url: https://clickhouse.com/
    keywords: [ database ]
    description: |
      Get the performance you love from open source ClickHouse in a serverless offering that takes care of the details so you can spend more time getting insight out of the fastest database on earth.
    icon: https://rook.io/images/rook-logo.svg
    docs: |
      ClickHouse® is an open-source column-oriented database management system that allows generating analytical data reports in real-time.
      
      ## Useful Links
      
      * [Official website](https://clickhouse.com/) has a quick high-level overview of ClickHouse on the main page.
      * [ClickHouse Cloud](https://clickhouse.cloud) ClickHouse as a service, built by the creators and maintainers.
      * [Tutorial](https://clickhouse.com/docs/en/getting_started/tutorial/) shows how to set up and query a small ClickHouse cluster.
      * [Documentation](https://clickhouse.com/docs/en/) provides more in-depth information.
      * [YouTube channel](https://www.youtube.com/c/ClickHouseDB) has a lot of content about ClickHouse in video format.
      * [Slack](https://join.slack.com/t/clickhousedb/shared_invite/zt-rxm3rdrk-lIUmhLC3V8WTaL0TGxsOmg) and [Telegram](https://telegram.me/clickhouse_en) allow chatting with ClickHouse users in real-time.
      * [Blog](https://clickhouse.com/blog/) contains various ClickHouse-related articles, as well as announcements and reports about events.
      * [Code Browser (Woboq)](https://clickhouse.com/codebrowser/ClickHouse/index.html) with syntax highlight and navigation.
      * [Code Browser (github.dev)](https://github.dev/ClickHouse/ClickHouse) with syntax highlight, powered by github.dev.
      * [Contacts](https://clickhouse.com/company/contact) can help to get your questions answered if there are any.
      
      ## Upcoming events
      * [**v22.11 Release Webinar**](https://clickhouse.com/company/events/v22-11-release-webinar) Original creator, co-founder, and CTO of ClickHouse Alexey Milovidov will walk us through the highlights of the release, provide live demos, and share vision into what is coming in the roadmap.
      * [**ClickHouse Meetup at the Deutsche Bank office in Berlin**](https://www.meetup.com/clickhouse-berlin-user-group/events/289311596/) Hear from Deutsche Bank on why they chose ClickHouse for big sensitive data in a regulated environment. The ClickHouse team will then present how ClickHouse is used for real time financial data analytics, including tick data, trade analytics and risk management.
      * [**AWS re:Invent**](https://clickhouse.com/company/events/aws-reinvent) Core members of the ClickHouse team -- including 2 of our founders -- will be at re:Invent from November 29 to December 3. We are available on the show floor, but are also determining interest in holding an event during the time there.
    ID: Unkonw
    arch: Unkonw
---
apiVersion: imagehub.sealos.io/v1
kind: Image
metadata:
  name: labring.redis-operator.3.1.4
spec:
  name: labring/redis-operator:3.1.4
  detail:
    url: https://docs.opstreelabs.in/redis-operator/
    keywords: [ Database , operator ]
    description: |
      A Golang based redis operator that will make/oversee Redis standalone/cluster mode setup on top of the Kubernetes. It can create a redis cluster setup with best practices on Cloud as well as the Bare metal environment. Also, it provides an in-built monitoring capability using redis-exporter.
    icon: https://2215807951-files.gitbook.io/~/files/v0/b/gitbook-legacy-files/o/assets%2F-MFROMngQbx5IGrLZR-g%2F-MFROP5SFffhX9AX-LAR%2F-MFROuWngEXGngu4yCyr%2Fimage.png?alt=media&token=c1561fe4-9740-4f34-99e2-bee924100fe7
    docs: |
      # redis-operator

      [![Build Status](https://github.com/spotahome/redis-operator/actions/workflows/ci.yaml/badge.svg?branch=master)](https://github.com/spotahome/redis-operator)
      [![Go Report Card](https://goreportcard.com/badge/github.com/spotahome/redis-operator)](https://goreportcard.com/report/github.com/spotahome/redis-operator)

      Redis Operator creates/configures/manages redis-failovers atop Kubernetes.

      ## Requirements

      Kubernetes version: 1.21 or higher
      Redis version: 6 or higher

      Redis operator is being tested against kubernetes 1.22 1.23 1.24 and redis 6
      All dependencies have been vendored, so there's no need to any additional download.

      ## Operator deployment on Kubernetes

      In order to create Redis failovers inside a Kubernetes cluster, the operator has to be deployed.
      It can be done with plain old [deployment](example/operator), using [Kustomize](manifests/kustomize) or with the provided [Helm chart](charts/redisoperator).

      ### Using the Helm chart

      From the root folder of the project, execute the following:

      ```
      helm repo add redis-operator https://spotahome.github.io/redis-operator
      helm repo update
      helm install redis-operator redis-operator/redis-operator
      ```

      #### Update helm chart

      Helm chart only manage the creation of CRD in the first install. In order to update the CRD you will need to apply directly.

      ```
      REDIS_OPERATOR_VERSION=v1.2.2
      kubectl replace -f https://raw.githubusercontent.com/spotahome/redis-operator/${REDIS_OPERATOR_VERSION}/manifests/databases.spotahome.com_redisfailovers.yaml
      ```

      ```
      helm upgrade redis-operator redis-operator/redis-operator
      ```
      ### Using kubectl

      To create the operator, you can directly create it with kubectl:

      ```
      REDIS_OPERATOR_VERSION=v1.2.2
      kubectl create -f https://raw.githubusercontent.com/spotahome/redis-operator/${REDIS_OPERATOR_VERSION}/manifests/databases.spotahome.com_redisfailovers.yaml
      kubectl apply -f https://raw.githubusercontent.com/spotahome/redis-operator/${REDIS_OPERATOR_VERSION}/example/operator/all-redis-operator-resources.yaml
      ```

      This will create a deployment named `redisoperator`.

      ### Using kustomize

      The kustomize setup included in this repo is highly customizable using [components](https://kubectl.docs.kubernetes.io/guides/config_management/components/),
      but it also comes with a few presets (in the form of overlays) supporting the most common use cases.

      To install the operator with default settings and every necessary resource (including RBAC, service account, default resource limits, etc), install the `default` overlay:

      ```shell
      kustomize build github.com/spotahome/redis-operator/manifests/kustomize/overlays/default
      ```

      If you would like to customize RBAC or the service account used, you can install the `minimal` overlay.

      Finally, you can install the `full` overlay if you want everything this operator has to offer, including Prometheus ServiceMonitor resources.

      It's always a good practice to pin the version of the operator in your configuration to make sure you are not surprised by changes on the latest development branch:

      ```shell
      kustomize build github.com/spotahome/redis-operator/manifests/kustomize/overlays/default?ref=v1.2.2
      ```

      You can easily create your own config by creating a `kustomization.yaml` file
      (for example to apply custom resource limits, to add custom labels or to customize the namespace):

      ```yaml
      apiVersion: kustomize.config.k8s.io/v1beta1
      kind: Kustomization

      namespace: redis-operator

      commonLabels:
          foo: bar

      resources:
        - github.com/spotahome/redis-operator/manifests/kustomize/overlays/full
      ```

      Take a look at the manifests inside [manifests/kustomize](manifests/kustomize) for more details.

      ## Usage

      Once the operator is deployed inside a Kubernetes cluster, a new API will be accesible, so you'll be able to create, update and delete redisfailovers.

      In order to deploy a new redis-failover a [specification](example/redisfailover/basic.yaml) has to be created:

      ```
      REDIS_OPERATOR_VERSION=v1.2.2
      kubectl create -f https://raw.githubusercontent.com/spotahome/redis-operator/${REDIS_OPERATOR_VERSION}/example/redisfailover/basic.yaml
      ```

      This redis-failover will be managed by the operator, resulting in the following elements created inside Kubernetes:

      - `rfr-<NAME>`: Redis configmap
      - `rfr-<NAME>`: Redis statefulset
      - `rfr-<NAME>`: Redis service (if redis-exporter is enabled)
      - `rfs-<NAME>`: Sentinel configmap
      - `rfs-<NAME>`: Sentinel deployment
      - `rfs-<NAME>`: Sentinel service

      **NOTE**: `NAME` is the named provided when creating the RedisFailover.
      **IMPORTANT**: the name of the redis-failover to be created cannot be longer that 48 characters, due to prepend of redis/sentinel identification and statefulset limitation.

      ### Persistence

      The operator has the ability of add persistence to Redis data. By default an `emptyDir` will be used, so the data is not saved.

      In order to have persistence, a `PersistentVolumeClaim` usage is allowed. The full [PVC definition has to be added](example/redisfailover/persistent-storage.yaml) to the Redis Failover Spec under the `Storage` section.

      **IMPORTANT**: By default, the persistent volume claims will be deleted when the Redis Failover is. If this is not the expected usage, a `keepAfterDeletion` flag can be added under the `storage` section of Redis. [An example is given](example/redisfailover/persistent-storage-no-pvc-deletion.yaml).

      ### NodeAffinity and Tolerations

      You can use NodeAffinity and Tolerations to deploy Pods to isolated groups of Nodes. Examples are given for [node affinity](example/redisfailover/node-affinity.yaml), [pod anti affinity](example/redisfailover/pod-anti-affinity.yaml) and [tolerations](example/redisfailover/tolerations.yaml).

      ## Topology Spread Contraints

      You can use the `topologySpreadContraints` to ensure the pods of a type(redis or sentinel) are evenly distributed across zones/nodes. Examples are for using [topology spread constraints](example/redisfailover/topology-spread-contraints.yaml). Further document on how `topologySpreadConstraints` work could be found [here](https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/).

      ### Custom configurations

      It is possible to configure both Redis and Sentinel. This is done with the `customConfig` option inside their spec. It is a list of configurations and their values. Example are given in the [custom config example file](example/redisfailover/custom-config.yaml).

      In order to have the ability of this configurations to be changed "on the fly", without the need of reload the redis/sentinel processes, the operator will apply them with calls to the redises/sentinels, using `config set` or `sentinel set mymaster` respectively. Because of this, **no changes on the configmaps** will appear regarding this custom configurations and the entries of `customConfig` from Redis spec will not be written on `redis.conf` file. To verify the actual Redis configuration use [`redis-cli CONFIG GET *`](https://redis.io/commands/config-get).

      **Important**: in the Sentinel options, there are some "conversions" to be made:

      - Configuration on the `sentinel.conf`: `sentinel down-after-milliseconds mymaster 2000`
      - Configuration on the `configOptions`: `down-after-milliseconds 2000`

      **Important 2**: do **NOT** change the options used for control the redis/sentinel such as `port`, `bind`, `dir`, etc.

      ### Custom shutdown script

      By default, a custom shutdown file is given. This file makes redis to `SAVE` it's data, and in the case that redis is master, it'll call sentinel to ask for a failover.

      This behavior is configurable, creating a configmap and indicating to use it. An example about how to use this option can be found on the [shutdown example file](example/redisfailover/custom-shutdown.yaml).

      **Important**: the configmap has to be in the same namespace. The configmap has to have a `shutdown.sh` data, containing the script.

      ### Custom SecurityContext

      By default Kubernetes will run containers as the user specified in the Dockerfile (or the root user if not specified), this is not always desirable.
      If you need the containers to run as a specific user (or provide any other PodSecurityContext options) then you can specify a custom `securityContext` in the
      `redisfailover` object. See the [SecurityContext example file](example/redisfailover/security-context.yaml) for an example. You can visit kubernetes documentation for detailed docs about [security context](https://kubernetes.io/docs/tasks/configure-pod-container/security-context/)

      ### Custom containerSecurityContext at container level

      By default Kubernetes will run containers with default docker capabilities for exemple, this is not always desirable.
      If you need the containers to run with specific capabilities or with read only root file system (or provide any other securityContext options) then you can specify a custom `containerSecurityContext` in the
      `redisfailover` object. See the [ContainerSecurityContext example file](example/redisfailover/container-security-context.yaml) for an example. Keys available under containerSecurityContext are detailed [here](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.20/#securitycontext-v1-core)

      ### Custom command

      By default, redis and sentinel will be called with the basic command, giving the configuration file:

      - Redis: `redis-server /redis/redis.conf`
      - Sentinel: `redis-server /redis/sentinel.conf --sentinel`

      If necessary, this command can be changed with the `command` option inside redis/sentinel spec. An example can be found in the [custom command example file](example/redisfailover/custom-command.yaml).

      ### Custom Priority Class
      In order to use a custom Kubernetes [Priority Class](https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/#priorityclass) for Redis and/or Sentinel pods, you can set the `priorityClassName` in the redis/sentinel spec, this attribute has no default and depends on the specific cluster configuration. **Note:** the operator doesn't create the referenced `Priority Class` resource.

      ### Custom Service Account
      In order to use a custom Kubernetes [Service Account](https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/) for Redis and/or Sentinel pods, you can set the `serviceAccountName` in the redis/sentinel spec, if not specified the `default` Service Account will be used. **Note:** the operator doesn't create the referenced `Service Account` resource.

      ### Custom Pod Annotations
      By default, no pod annotations will be applied to Redis nor Sentinel pods.

      In order to apply custom pod Annotations, you can provide the `podAnnotations` option inside redis/sentinel spec. An example can be found in the [custom annotations example file](example/redisfailover/custom-annotations.yaml).
      ### Custom Service Annotations
      By default, no service annotations will be applied to the Redis nor Sentinel services.

      In order to apply custom service Annotations, you can provide the `serviceAnnotations` option inside redis/sentinel spec. An example can be found in the [custom annotations example file](example/redisfailover/custom-annotations.yaml).

      ### Control of label propagation.
      By default the operator will propagate all labels on the CRD down to the resources that it creates.  This can be problematic if the
      labels on the CRD are not fully under your own control (for example: being deployed by a gitops operator)
      as a change to a labels value can fail on immutable resources such as PodDisruptionBudgets.  To control what labels the operator propagates
      to resource is creates you can modify the labelWhitelist option in the spec.

      By default specifying no whitelist or an empty whitelist will cause all labels to still be copied as not to break backwards compatibility.

      Items in the array should be regular expressions, see [here](example/redisfailover/control-label-propagation.yaml) as an example of how they can be used and
      [here](https://github.com/google/re2/wiki/Syntax) for a syntax reference.

      The whitelist can also be used as a form of blacklist by specifying a regular expression that will not match any label.

      NOTE: The operator will always add the labels it requires for operation to resources.  These are the following:
      ```
      app.kubernetes.io/component
      app.kubernetes.io/managed-by
      app.kubernetes.io/name
      app.kubernetes.io/part-of
      redisfailovers.databases.spotahome.com/name
      ```


      ### ExtraVolumes and ExtraVolumeMounts

      If the user choose to have extra volumes creates and mounted, he could use the `extraVolumes` and `extraVolumeMounts`, in `spec.redis` of the CRD. This allows users to mount the extra configurations, or secrets to be used. A typical use case for this might be
      - Secrets that sidecars might use to backup of RDBs
      - Extra users and their secrets and acls that could used the initContainers to create multiple users
      - Extra Configurations that could merge on top the existing configurations
      - To pass failover scripts for addition for additional operations

      ```
      ---
      apiVersion: v1
      kind: Secret
      metadata:
        name: foo
        namespace: exm
      type: Opaque
      stringData:
        password: MWYyZDFlMmU2N2Rm
      ---
      apiVersion: databases.spotahome.com/v1
      kind: RedisFailover
      metadata:
        name: foo
        namespace: exm
      spec:
        sentinel:
          replicas: 3
          extraVolumes:
          - name: foo
            secret:
              secretName: foo
              optional: false
          extraVolumeMounts:
          - name: foo
            mountPath: "/etc/foo"
            readOnly: true
        redis:
          replicas: 3
          extraVolumes:
          - name: foo
            secret:
              secretName: foo
              optional: false
          extraVolumeMounts:
          - name: foo
            mountPath: "/etc/foo"
            readOnly: true
      ```



      ## Connection to the created Redis Failovers

      In order to connect to the redis-failover and use it, a [Sentinel-ready](https://redis.io/topics/sentinel-clients) library has to be used. This will connect through the Sentinel service to the Redis node working as a master.
      The connection parameters are the following:

      ```
      url: rfs-<NAME>
      port: 26379
      master-name: mymaster
      ```

      ### Enabling redis auth

      To enable auth create a secret with a password field:

      ```
      echo -n "pass" > password
      kubectl create secret generic redis-auth --from-file=password

      ## example config
      apiVersion: databases.spotahome.com/v1
      kind: RedisFailover
      metadata:
        name: redisfailover
      spec:
        sentinel:
          replicas: 3
        redis:
          replicas: 1
        auth:
          secretPath: redis-auth
      ```
      You need to set secretPath as the secret name which is created before.

      ### Bootstrapping from pre-existing Redis Instance(s)
      If you are wanting to migrate off of a pre-existing Redis instance, you can provide a `bootstrapNode` to your `RedisFailover` resource spec.

      This `bootstrapNode` can be configured as follows:
      |       Key      | Type         | Description                                                                                                                                                                               | Example File                                                                                 |
      |:--------------:|--------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------|
      | host           | **required** | The IP of the target Redis address or the ClusterIP of a pre-existing Kubernetes Service targeting Redis pods                                                                             | [bootstrapping.yaml](example/redisfailover/bootstrapping.yaml)                               |
      | port           | _optional_   | The Port that the target Redis address is listening to. Defaults to `6379`.                                                                                                               | [bootstrapping-with-port.yaml](example/redisfailover/bootstrapping-with-port.yaml)           |
      | allowSentinels | _optional_   | Allow the Operator to also create the specified Sentinel resources and point them to the target Node/Port. By default, the Sentinel resources will **not** be created when bootstrapping. | [bootstrapping-with-sentinels.yaml](example/redisfailover/bootstrapping-with-sentinels.yaml) |

      #### What is Bootstrapping?
      When a `bootstrapNode` is provided, the Operator will always set all of the defined Redis instances to replicate from the provided `bootstrapNode` host value.
      This allows for defining a `RedisFailover` that replicates from an existing Redis instance to ease cutover from one instance to another.

      **Note: Redis instance will always be configured with `replica-priority 0`. This means that these Redis instances can _never_ be promoted to a `master`.**

      Depending on the configuration provided, the Operator will launch the `RedisFailover` in two bootstrapping states: without sentinels and with sentinels.

      #### Default Bootstrapping Mode (Without Sentinels)
      By default, if the `RedisFailover` resource defines a valid `bootstrapNode`, **only the redis instances will be created**.
      This allows for ease of bootstrapping from an existing `RedisFailover` instance without the Sentinels intermingling with each other.

      #### Bootstrapping With Sentinels
      When `allowSentinels` is provided, the Operator will also create the defined Sentinel resources. These sentinels will be configured to point to the provided
      `bootstrapNode` as their monitored master.

      ### Default versions

      The image versions deployed by the operator can be found on the [defaults file](api/redisfailover/v1/defaults.go).
      ## Cleanup

      ### Operator and CRD

      If you want to delete the operator from your Kubernetes cluster, the operator deployment should be deleted.

      Also, the CRD has to be deleted. Deleting CRD automatically wil delete all redis failover custom resources and their managed resources:

      ```
      kubectl delete crd redisfailovers.databases.spotahome.com
      ```

      ### Single Redis Failover

      Thanks to Kubernetes' `OwnerReference`, all the objects created from a redis-failover will be deleted after the custom resource is.

      ```
      kubectl delete redisfailover <NAME>
      ```

      ## Docker Images

      ### Redis Operator

      [![Redis Operator Image](https://quay.io/repository/spotahome/redis-operator/status "Redis Operator Image")](https://quay.io/repository/spotahome/redis-operator)
      ## Documentation

      For the code documentation, you can lookup on the [GoDoc](https://godoc.org/github.com/spotahome/redis-operator).

      Also, you can check more deeply information on the [docs folder](docs).
    ID: Unkonw
    arch: Unkonw
---
apiVersion: imagehub.sealos.io/v1
kind: Image
metadata:
  name: labring.kube-prometheus-stack.v0.56.0
spec:
  name: labring/kube-prometheus-stack:v0.56.0
  detail:
    url: https://artifacthub.io/packages/helm/prometheus-worawutchan/kube-prometheus-stack
    keywords: [ Monitoring ]
    description: |
      Installs the kube-prometheus stack, a collection of Kubernetes manifests, Grafana dashboards, and Prometheus rules combined with documentation and scripts to provide easy to operate end-to-end Kubernetes cluster monitoring with Prometheus using the Prometheus Operator.
    icon: https://artifacthub.io/image/0503add5-3fce-4b63-bbf3-b9f649512a86@1x
    docs: |
      # Prometheus Community Kubernetes Helm Charts

      [![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0) ![Release Charts](https://github.com/prometheus-community/helm-charts/workflows/Release%20Charts/badge.svg?branch=main) [![Releases downloads](https://img.shields.io/github/downloads/prometheus-community/helm-charts/total.svg)](https://github.com/prometheus-community/helm-charts/releases)

      This functionality is in beta and is subject to change. The code is provided as-is with no warranties. Beta features are not subject to the support SLA of official GA features.

      ## Usage

      [Helm](https://helm.sh) must be installed to use the charts.
      Please refer to Helm's [documentation](https://helm.sh/docs/) to get started.

      Once Helm is set up properly, add the repo as follows:

      ```console
      helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
      ```

      You can then run `helm search repo prometheus-community` to see the charts.

      ## Contributing

      The source code of all [Prometheus](https://prometheus.io) community [Helm](https://helm.sh) charts can be found on Github: <https://github.com/prometheus-community/helm-charts/>

      <!-- Keep full URL links to repo files because this README syncs from main to gh-pages.  -->
      We'd love to have you contribute! Please refer to our [contribution guidelines](https://github.com/prometheus-community/helm-charts/blob/main/CONTRIBUTING.md) for details.

      ## License

      <!-- Keep full URL links to repo files because this README syncs from main to gh-pages.  -->
      [Apache 2.0 License](https://github.com/prometheus-community/helm-charts/blob/main/LICENSE).

      ## Helm charts build status

      ![Release Charts](https://github.com/prometheus-community/helm-charts/workflows/Release%20Charts/badge.svg?branch=main)
    ID: Unkonw
    arch: Unkonw
---
apiVersion: imagehub.sealos.io/v1
kind: Image
metadata:
  name: labring.metrics-server.v0.6.1
spec:
  name: labring/metrics-server:v0.6.1
  detail:
    url: https://github.com/kubernetes-sigs/metrics-server
    keywords: [ Monitoring ]
    description: |
      Metrics Server is a scalable, efficient source of container resource metrics for Kubernetes built-in autoscaling pipelines.
    icon: https://dyltqmyl993wv.cloudfront.net/assets/stacks/metrics-server/img/metrics-server-stack-220x234.png
    docs: |
      # Kubernetes Metrics Server

      Metrics Server is a scalable, efficient source of container resource metrics for Kubernetes
      built-in autoscaling pipelines.

      Metrics Server collects resource metrics from Kubelets and exposes them in Kubernetes apiserver through [Metrics API] 
      for use by [Horizontal Pod Autoscaler] and [Vertical Pod Autoscaler]. Metrics API can also be accessed by `kubectl top`,
      making it easier to debug autoscaling pipelines.

      Metrics Server is not meant for non-autoscaling purposes. For example, don't use it to forward metrics to monitoring solutions, or as a source of monitoring solution metrics. In such cases please collect metrics from Kubelet `/metrics/resource` endpoint directly.

      Metrics Server offers:
      - A single deployment that works on most clusters (see [Requirements](#requirements))
      - Fast autoscaling, collecting metrics every 15 seconds.
      - Resource efficiency, using 1 mili core of CPU and 2 MB of memory for each node in a cluster.
      - Scalable support up to 5,000 node clusters.

      [Metrics API]: https://github.com/kubernetes/metrics
      [Horizontal Pod Autoscaler]: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/
      [Vertical Pod Autoscaler]: https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler/

      ## Use cases

      You can use Metrics Server for:
      - CPU/Memory based horizontal autoscaling (learn more about [Horizontal Autoscaling])
      - Automatically adjusting/suggesting resources needed by containers (learn more about [Vertical Autoscaling])

      Don't use Metrics Server when you need:
      - Non-Kubernetes clusters
      - An accurate source of resource usage metrics
      - Horizontal autoscaling based on other resources than CPU/Memory

      For unsupported use cases, check out full monitoring solutions like Prometheus.

      [Horizontal Autoscaling]: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/
      [Vertical Autoscaling]: https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler/

      ## Requirements

      Metrics Server has specific requirements for cluster and network configuration. These requirements aren't the default for all cluster
      distributions. Please ensure that your cluster distribution supports these requirements before using Metrics Server:
      - The kube-apiserver must [enable an aggregation layer].
      - Nodes must have Webhook [authentication and authorization] enabled.
      - Kubelet certificate needs to be signed by cluster Certificate Authority (or disable certificate validation by passing `--kubelet-insecure-tls` to Metrics Server)
      - Container runtime must implement a [container metrics RPCs] (or have [cAdvisor] support)
      - Network should support following communication:
        - Control plane to Metrics Server. Control plane node needs to reach Metrics Server's pod IP and port 10250 (or node IP and custom port if `hostNetwork` is enabled). Read more about [control plane to node communication](https://kubernetes.io/docs/concepts/architecture/control-plane-node-communication/#control-plane-to-node). 
        - Metrics Server to Kubelet on all nodes. Metrics server needs to reach node address and Kubelet port. Addresses and ports are configured in Kubelet and published as part of Node object. Addresses in `.status.addresses` and port in `.status.daemonEndpoints.kubeletEndpoint.port` field (default 10250). Metrics Server will pick first node address based on the list provided by `kubelet-preferred-address-types` command line flag (default `InternalIP,ExternalIP,Hostname` in manifests). 

      [reachable from kube-apiserver]: https://kubernetes.io/docs/concepts/architecture/master-node-communication/#master-to-cluster
      [enable an aggregation layer]: https://kubernetes.io/docs/tasks/access-kubernetes-api/configure-aggregation-layer/
      [authentication and authorization]: https://kubernetes.io/docs/reference/access-authn-authz/kubelet-authn-authz/
      [container metrics RPCs]: https://github.com/kubernetes/community/blob/master/contributors/devel/sig-node/cri-container-stats.md
      [cAdvisor]: https://github.com/google/cadvisor

      ## Installation

      Metrics Server can be installed either directly from YAML manifest or via the official [Helm chart](https://artifacthub.io/packages/helm/metrics-server/metrics-server). To install the latest Metrics Server release from the _components.yaml_ manifest, run the following command.

      ```shell
      kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml
      ```

      Installation instructions for previous releases can be found in [Metrics Server releases](https://github.com/kubernetes-sigs/metrics-server/releases).

      ### Compatibility Matrix

      Metrics Server | Metrics API group/version | Supported Kubernetes version
      ---------------|---------------------------|-----------------------------
      0.6.x          | `metrics.k8s.io/v1beta1`  | 1.19+
      0.5.x          | `metrics.k8s.io/v1beta1`  | *1.8+
      0.4.x          | `metrics.k8s.io/v1beta1`  | *1.8+
      0.3.x          | `metrics.k8s.io/v1beta1`  | 1.8-1.21

      *Kubernetes versions lower than v1.16 require passing the `--authorization-always-allow-paths=/livez,/readyz` command line flag

      ### High Availability

      Metrics Server can be installed in high availability mode directly from a YAML manifest or via the official [Helm chart](https://artifacthub.io/packages/helm/metrics-server/metrics-server) by setting the `replicas` value greater than `1`. To install the latest Metrics Server release in high availability mode from the  _high-availability.yaml_ manifest, run the following command.

      ```shell
      kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/high-availability.yaml
      ```

      Note that this configuration **requires** having a cluster with at least 2 nodes on which Metrics Server can be scheduled.

      Also, to maximize the efficiency of this highly available configuration, it is **recommended** to add the `--enable-aggregator-routing=true` CLI flag to the kube-apiserver so that requests sent to Metrics Server are load balanced between the 2 instances.

      ### Helm Chart

      The [Helm chart](https://artifacthub.io/packages/helm/metrics-server/metrics-server) is maintained as an additional component within this repo and released into a chart repository backed on the `gh-pages` branch. A new version of the chart will be released for each Metrics Server release and can also be released independently if there is a need. The chart on the `master` branch shouldn't be referenced directly as it might contain modifications since it was last released, to view the chart code use the chart release tag.

      ## Security context

      Metrics Server requires the `CAP_NET_BIND_SERVICE` capability in order to bind to a privileged ports as non-root.
      If you are running Metrics Server in an environment that uses [PSSs](https://kubernetes.io/docs/concepts/security/pod-security-standards/) or other mechanisms to restrict pod capabilities, ensure that Metrics Server is allowed
      to use this capability.
      This applies even if you use the `--secure-port` flag to change the port that Metrics Server binds to a non-privileged port.

      ## Scaling

      Starting from v0.5.0 Metrics Server comes with default resource requests that should guarantee good performance for most cluster configurations up to 100 nodes:

      * 100m core of CPU
      * 200MiB of memory

      Metrics Server resource usage depends on multiple independent dimensions, creating a [Scalability Envelope].
      Default Metrics Server configuration should work in clusters that don't exceed any of the thresholds listed below:

      Quantity               | Namespace threshold | Cluster threshold
      -----------------------|---------------------|------------------
      #Nodes                 | n/a                 | 100
      #Pods per node         | 70                  | 70
      #Deployments with HPAs | 100                 | 100

      Resources can be adjusted proportionally based on number of nodes in the cluster.
      For clusters of more than 100 nodes, allocate additionally:
      * 1m core per node
      * 2MiB memory per node

      You can use the same approach to lower resource requests, but there is a boundary
      where this may impact other scalability dimensions like maximum number of pods per node.

      [Scalability Envelope]: https://github.com/kubernetes/community/blob/master/sig-scalability/configs-and-limits/thresholds.md

      ### Configuration 

      Depending on your cluster setup, you may also need to change flags passed to the Metrics Server container.
      Most useful flags:
      - `--kubelet-preferred-address-types` - The priority of node address types used when determining an address for connecting to a particular node (default [Hostname,InternalDNS,InternalIP,ExternalDNS,ExternalIP])
      - `--kubelet-insecure-tls` - Do not verify the CA of serving certificates presented by Kubelets. For testing purposes only.
      - `--requestheader-client-ca-file` - Specify a root certificate bundle for verifying client certificates on incoming requests.

      You can get a full list of Metrics Server configuration flags by running:

      ```shell
      docker run --rm registry.k8s.io/metrics-server/metrics-server:v0.6.0 --help
      ```

      ## Design

      Metrics Server is a component in the core metrics pipeline described in [Kubernetes monitoring architecture].

      For more information, see:
      - [Metrics API design]
      - [Metrics Server design]

      [Kubernetes monitoring architecture]: https://github.com/kubernetes/design-proposals-archive/blob/main/instrumentation/monitoring_architecture.md
      [Metrics API design]: https://github.com/kubernetes/design-proposals-archive/blob/main/instrumentation/resource-metrics-api.md
      [Metrics Server design]: https://github.com/kubernetes/design-proposals-archive/blob/main/instrumentation/metrics-server.md

      ## Have a question?

      Before posting an issue, first checkout [Frequently Asked Questions] and [Known Issues].

      [Frequently Asked Questions]: FAQ.md
      [Known Issues]: KNOWN_ISSUES.md

      ## Community, discussion, contribution, and support

      Learn how to engage with the Kubernetes community on the [community page].

      You can reach the maintainers of this project at:

      - [Slack channel]
      - [Mailing list]

      This project is maintained by [SIG Instrumentation]

      [community page]: http://kubernetes.io/community/
      [Slack channel]: https://kubernetes.slack.com/messages/sig-instrumentation
      [Mailing list]: https://groups.google.com/forum/#!forum/kubernetes-sig-instrumentation
      [SIG Instrumentation]: https://github.com/kubernetes/community/tree/master/sig-instrumentation

      ### Code of conduct

      Participation in the Kubernetes community is governed by the [Kubernetes Code of Conduct].

      [Kubernetes Code of Conduct]: code-of-conduct.md
    ID: Unkonw
    arch: Unkonw
---
apiVersion: imagehub.sealos.io/v1
kind: Image
metadata:
  name: labring.redis-exporter.latest
spec:
  name: labring/redis-exporter:latest
  detail:
    url: https://exporterhub.io/exporter/redis-exporter/
    keywords: [ Monitoring ]
    description: |
      Redis stands for Remote Dictionary Server and is an in-memory data structure store used as a database, cache, streaming engine, and message broker. It provides data structures such as strings, hashes, lists, sets, sorted sets with range queries, bitmaps, geospatial indexes, and streams. Redis has built-in replication, Lua scripting, LRU eviction, transactions, and different levels of on-disk persistence, and provides high availability via Redis Sentinel and automatic partitioning with Redis Cluster.

      A Redis exporter is required to monitor and expose Redis' metrics. It queries Redis, scraps the data, and exposes the metrics to a Kubernetes service endpoint that can further be scrapped by Prometheus to ingest the time series data. For monitoring Redis, we use an external Prometheus exporter, which is maintained by the Prometheus Community. On deployment, this exporter scraps sizable metrics from Redis and helps users get crucial information that is difficult to get from Redis directly and continuously. 

      For this setup, we are using bitnami redis Helm charts to start the Redis server/cluster.
    icon: https://i0.wp.com/exporterhub.io/wp-content/uploads/2022/06/redis-cube-red_white-rgb.png?fit=294%2C252&ssl=1
    docs: |
      # Prometheus Redis Metrics Exporter

      [![Build Status](https://cloud.drone.io/api/badges/oliver006/redis_exporter/status.svg)](https://cloud.drone.io/oliver006/redis_exporter)
       [![Coverage Status](https://coveralls.io/repos/github/oliver006/redis_exporter/badge.svg?branch=master)](https://coveralls.io/github/oliver006/redis_exporter?branch=master) [![codecov](https://codecov.io/gh/oliver006/redis_exporter/branch/master/graph/badge.svg)](https://codecov.io/gh/oliver006/redis_exporter) [![docker_pulls](https://img.shields.io/docker/pulls/oliver006/redis_exporter.svg)](https://img.shields.io/docker/pulls/oliver006/redis_exporter.svg) [![Stand With Ukraine](https://raw.githubusercontent.com/vshymanskyy/StandWithUkraine/main/badges/StandWithUkraine.svg)](https://stand-with-ukraine.pp.ua)

      Prometheus exporter for Redis metrics.\
      Supports Redis 2.x, 3.x, 4.x, 5.x, 6.x, and 7.x

      #### Ukraine is currently suffering from Russian aggression, [please consider supporting Ukraine with a donation](https://www.supportukraine.co/).

      [![Stand With Ukraine](https://raw.githubusercontent.com/vshymanskyy/StandWithUkraine/main/banner2-direct.svg)](https://stand-with-ukraine.pp.ua)


      ## Building and running the exporter

      ### Build and run locally

      ```sh
      git clone https://github.com/oliver006/redis_exporter.git
      cd redis_exporter
      go build .
      ./redis_exporter --version
      ```


      ### Pre-build binaries

      For pre-built binaries please take a look at [the releases](https://github.com/oliver006/redis_exporter/releases).


      ### Basic Prometheus Configuration

      Add a block to the `scrape_configs` of your prometheus.yml config file:

      ```yaml
      scrape_configs:
        - job_name: redis_exporter
          static_configs:
          - targets: ['<<REDIS-EXPORTER-HOSTNAME>>:9121']
      ```

      and adjust the host name accordingly.


      ### Kubernetes SD configurations

      To have instances in the drop-down as human readable names rather than IPs, it is suggested to use [instance relabelling](https://www.robustperception.io/controlling-the-instance-label).

      For example, if the metrics are being scraped via the pod role, one could add:

      ```yaml
                - source_labels: [__meta_kubernetes_pod_name]
                  action: replace
                  target_label: instance
                  regex: (.*redis.*)
      ```

      as a relabel config to the corresponding scrape config. As per the regex value, only pods with "redis" in their name will be relabelled as such.

      Similar approaches can be taken with [other role types](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#kubernetes_sd_config) depending on how scrape targets are retrieved.


      ### Prometheus Configuration to Scrape Multiple Redis Hosts

      The Prometheus docs have a [very informative article](https://prometheus.io/docs/guides/multi-target-exporter/) on how multi-target exporters are intended to work.  

      Run the exporter with the command line flag `--redis.addr=` so it won't try to access the local instance every time the `/metrics` endpoint is scraped. Using below config instead of the /metric endpoint the /scrape endpoint will be used by prometheus. As an example the first target will be queried with this web request:
      http://exporterhost:9121/scrape?target=first-redis-host:6379

      ```yaml
      scrape_configs:
        ## config for the multiple Redis targets that the exporter will scrape
        - job_name: 'redis_exporter_targets'
          static_configs:
            - targets:
              - redis://first-redis-host:6379
              - redis://second-redis-host:6379
              - redis://second-redis-host:6380
              - redis://second-redis-host:6381
          metrics_path: /scrape
          relabel_configs:
            - source_labels: [__address__]
              target_label: __param_target
            - source_labels: [__param_target]
              target_label: instance
            - target_label: __address__
              replacement: <<REDIS-EXPORTER-HOSTNAME>>:9121

        ## config for scraping the exporter itself
        - job_name: 'redis_exporter'
          static_configs:
            - targets:
              - <<REDIS-EXPORTER-HOSTNAME>>:9121
      ```

      The Redis instances are listed under `targets`, the Redis exporter hostname is configured via the last relabel_config rule.\
      If authentication is needed for the Redis instances then you can set the password via the `--redis.password` command line option of
      the exporter (this means you can currently only use one password across the instances you try to scrape this way. Use several
      exporters if this is a problem). \
      You can also use a json file to supply multiple targets by using `file_sd_configs` like so:

      ```yaml

      scrape_configs:
        - job_name: 'redis_exporter_targets'
          file_sd_configs:
            - files:
              - targets-redis-instances.json
          metrics_path: /scrape
          relabel_configs:
            - source_labels: [__address__]
              target_label: __param_target
            - source_labels: [__param_target]
              target_label: instance
            - target_label: __address__
              replacement: <<REDIS-EXPORTER-HOSTNAME>>:9121

        ## config for scraping the exporter itself
        - job_name: 'redis_exporter'
          static_configs:
            - targets:
              - <<REDIS-EXPORTER-HOSTNAME>>:9121
      ```

      The `targets-redis-instances.json` should look something like this:

      ```json
      [
        {
          "targets": [ "redis://redis-host-01:6379", "redis://redis-host-02:6379"],
          "labels": { }
        }
      ]
      ```

      Prometheus uses file watches and all changes to the json file are applied immediately.


      ### Command line flags

      | Name                    | Environment Variable Name              | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |
      |-------------------------|----------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
      | redis.addr              | REDIS_ADDR                             | Address of the Redis instance, defaults to `redis://localhost:6379`.                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
      | redis.user              | REDIS_USER                             | User name to use for authentication (Redis ACL for Redis 6.0 and newer).                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |
      | redis.password          | REDIS_PASSWORD                         | Password of the Redis instance, defaults to `""` (no password).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |
      | redis.password-file     | REDIS_PASSWORD_FILE                    | Password file of the Redis instance to scrape, defaults to `""` (no password file).                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
      | check-keys              | REDIS_EXPORTER_CHECK_KEYS              | Comma separated list of key patterns to export value and length/size, eg: `db3=user_count` will export key `user_count` from db `3`. db defaults to `0` if omitted. The key patterns specified with this flag will be found using [SCAN](https://redis.io/commands/scan).  Use this option if you need glob pattern matching; `check-single-keys` is faster for non-pattern keys. Warning: using `--check-keys` to match a very large number of keys can slow down the exporter to the point where it doesn't finish scraping the redis instance. |
      | check-single-keys       | REDIS_EXPORTER_CHECK_SINGLE_KEYS       | Comma separated list of keys to export value and length/size, eg: `db3=user_count` will export key `user_count` from db `3`. db defaults to `0` if omitted.  The keys specified with this flag will be looked up directly without any glob pattern matching.  Use this option if you don't need glob pattern matching;  it is faster than `check-keys`.                                                                                                                                                                                           |
      | check-streams           | REDIS_EXPORTER_CHECK_STREAMS           | Comma separated list of stream-patterns to export info about streams, groups and consumers. Syntax is the same as `check-keys`.                                                                                                                                                                                                                                                                                                                                                                                                                   |
      | check-single-streams    | REDIS_EXPORTER_CHECK_SINGLE_STREAMS    | Comma separated list of streams to export info about streams, groups and consumers. The streams specified with this flag will be looked up directly without any glob pattern matching.  Use this option if you don't need glob pattern matching;  it is faster than `check-streams`.                                                                                                                                                                                                                                                              |
      | check-keys-batch-size   | REDIS_EXPORTER_CHECK_KEYS_BATCH_SIZE   | Approximate number of keys to process in each execution. This is basically the COUNT option that will be passed into the SCAN command as part of the execution of the key or key group metrics, see [COUNT option](https://redis.io/commands/scan#the-count-option). Larger value speeds up scanning. Still Redis is a single-threaded app, huge `COUNT` can affect production environment.                                                                                                                                                       |
      | count-keys              | REDIS_EXPORTER_COUNT_KEYS              | Comma separated list of patterns to count, eg: `db3=sessions:*` will count all keys with prefix `sessions:` from db `3`. db defaults to `0` if omitted. Warning: The exporter runs SCAN to count the keys. This might not perform well on large databases.                                                                                                                                                                                                                                                                                        |
      | script                  | REDIS_EXPORTER_SCRIPT                  | Path to Redis Lua script for gathering extra metrics.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
      | debug                   | REDIS_EXPORTER_DEBUG                   | Verbose debug output                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
      | log-format              | REDIS_EXPORTER_LOG_FORMAT              | Log format, valid options are `txt` (default) and `json`.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
      | namespace               | REDIS_EXPORTER_NAMESPACE               | Namespace for the metrics, defaults to `redis`.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |
      | connection-timeout      | REDIS_EXPORTER_CONNECTION_TIMEOUT      | Timeout for connection to Redis instance, defaults to "15s" (in Golang duration format)                                                                                                                                                                                                                                                                                                                                                                                                                                                           |
      | web.listen-address      | REDIS_EXPORTER_WEB_LISTEN_ADDRESS      | Address to listen on for web interface and telemetry, defaults to `0.0.0.0:9121`.                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
      | web.telemetry-path      | REDIS_EXPORTER_WEB_TELEMETRY_PATH      | Path under which to expose metrics, defaults to `/metrics`.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |
      | redis-only-metrics      | REDIS_EXPORTER_REDIS_ONLY_METRICS      | Whether to also export go runtime metrics, defaults to false.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |
      | include-config-metrics  | REDIS_EXPORTER_INCL_CONFIG_METRICS     | Whether to include all config settings as metrics, defaults to false.                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
      | include-system-metrics  | REDIS_EXPORTER_INCL_SYSTEM_METRICS     | Whether to include system metrics like `total_system_memory_bytes`, defaults to false.                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
      | redact-config-metrics   | REDIS_EXPORTER_REDACT_CONFIG_METRICS   | Whether to redact config settings that include potentially sensitive information like passwords.                                                                                                                                                                                                                                                                                                                                                                                                                                                  |
      | ping-on-connect         | REDIS_EXPORTER_PING_ON_CONNECT         | Whether to ping the redis instance after connecting and record the duration as a metric, defaults to false.                                                                                                                                                                                                                                                                                                                                                                                                                                       |
      | is-tile38               | REDIS_EXPORTER_IS_TILE38               | Whether to scrape Tile38 specific metrics, defaults to false.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |
      | is-cluster              | REDIS_EXPORTER_IS_CLUSTER              | Whether this is a redis cluster (Enable this if you need to fetch key level data on a Redis Cluster).                                                                                                                                                                                                                                                                                                                                                                                                                                             |
      | export-client-list      | REDIS_EXPORTER_EXPORT_CLIENT_LIST      | Whether to scrape Client List specific metrics, defaults to false.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |
      | export-client-port      | REDIS_EXPORTER_EXPORT_CLIENT_PORT      | Whether to include the client's port when exporting the client list. Warning: including the port increases the number of metrics generated and will make your Prometheus server take up more memory                                                                                                                                                                                                                                                                                                                                               |
      | skip-tls-verification   | REDIS_EXPORTER_SKIP_TLS_VERIFICATION   | Whether to skip TLS verification                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
      | tls-client-key-file     | REDIS_EXPORTER_TLS_CLIENT_KEY_FILE     | Name of the client key file (including full path) if the server requires TLS client authentication                                                                                                                                                                                                                                                                                                                                                                                                                                                |
      | tls-client-cert-file    | REDIS_EXPORTER_TLS_CLIENT_CERT_FILE    | Name the client cert file (including full path) if the server requires TLS client authentication                                                                                                                                                                                                                                                                                                                                                                                                                                                  |
      | tls-server-key-file     | REDIS_EXPORTER_TLS_SERVER_KEY_FILE     | Name of the server key file (including full path) if the web interface and telemetry should use TLS                                                                                                                                                                                                                                                                                                                                                                                                                                               |
      | tls-server-cert-file    | REDIS_EXPORTER_TLS_SERVER_CERT_FILE    | Name of the server certificate file (including full path) if the web interface and telemetry should use TLS                                                                                                                                                                                                                                                                                                                                                                                                                                       |
      | tls-server-ca-cert-file | REDIS_EXPORTER_TLS_SERVER_CA_CERT_FILE | Name of the CA certificate file (including full path) if the web interface and telemetry should use TLS                                                                                                                                                                                                                                                                                                                                                                                                                 |
      | tls-server-min-version  | REDIS_EXPORTER_TLS_SERVER_MIN_VERSION  | Minimum TLS version that is acceptable by the web interface and telemetry when using TLS, defaults to `TLS1.2` (supports `TLS1.0`,`TLS1.1`,`TLS1.2`,`TLS1.3`).                                                                                                                                                                                                                                                                                                                                                                                    |
      | tls-ca-cert-file        | REDIS_EXPORTER_TLS_CA_CERT_FILE        | Name of the CA certificate file (including full path) if the server requires TLS client authentication                                                                                                                                                                                                                                                                                                                                                                                                                                            |
      | set-client-name         | REDIS_EXPORTER_SET_CLIENT_NAME         | Whether to set client name to redis_exporter, defaults to true.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |
      | check-key-groups        | REDIS_EXPORTER_CHECK_KEY_GROUPS        | Comma separated list of [LUA regexes](https://www.lua.org/pil/20.1.html) for classifying keys into groups. The regexes are applied in specified order to individual keys, and the group name is generated by concatenating all capture groups of the first regex that matches a key. A key will be tracked under the `unclassified` group if none of the specified regexes matches it.                                                                                                                                                            |
      | max-distinct-key-groups | REDIS_EXPORTER_MAX_DISTINCT_KEY_GROUPS | Maximum number of distinct key groups that can be tracked independently *per Redis database*. If exceeded, only key groups with the highest memory consumption within the limit will be tracked separately, all remaining key groups will be tracked under a single `overflow` key group.                                                                                                                                                                                                                                                         |
      | config-command          | REDIS_EXPORTER_CONFIG_COMMAND          | What to use for the CONFIG command, defaults to `CONFIG`.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |

      Redis instance addresses can be tcp addresses: `redis://localhost:6379`, `redis.example.com:6379` or e.g. unix sockets: `unix:///tmp/redis.sock`.\
      SSL is supported by using the `rediss://` schema, for example: `rediss://azure-ssl-enabled-host.redis.cache.windows.net:6380` (note that the port is required when connecting to a non-standard 6379 port, e.g. with Azure Redis instances).\

      Command line settings take precedence over any configurations provided by the environment variables.


      ### Authenticating with Redis

      If your Redis instance requires authentication then there are several ways how you can supply 
      a username (new in Redis 6.x with ACLs) and a password.

      You can provide the username and password as part of the address, see [here](https://www.iana.org/assignments/uri-schemes/prov/redis) for the official documentation of the `redis://` scheme.
      You can set `-redis.password-file=sample-pwd-file.json` to specify a password file, it's used whenever the exporter connects to a Redis instance,
      no matter if you're using the `/scrape` endpoint for multiple instances or the normal `/metrics` endpoint when scraping just one instance.
      It only takes effect when `redis.password == ""`.  See the [contrib/sample-pwd-file.json](contrib/sample-pwd-file.json) for a working example, and make sure to always include the `redis://` in your password file entries.

      An example for a URI including a password is: `redis://<<username (optional)>>:<<PASSWORD>>@<<HOSTNAME>>:<<PORT>>`

      Alternatively, you can provide the username and/or password using the `--redis.user` and `--redis.password` directly to the redis_exporter.

      If you want to use a dedicated Redis user for the redis_exporter (instead of the default user) then you need enable a list of commands for that user.
      You can use the following Redis command to set up the user, just replace `<<<USERNAME>>>` and `<<<PASSWORD>>>` with your desired values.
      ```
      ACL SETUSER <<<USERNAME>>> +client +ping +info +config|get +cluster|info +slowlog +latency +memory +select +get +scan +xinfo +type +pfcount +strlen +llen +scard +zcard +hlen +xlen +eval allkeys on > <<<PASSWORD>>>
      ```


      ### Run via Docker

      The latest release is automatically published to the [Docker registry](https://hub.docker.com/r/oliver006/redis_exporter/).

      You can run it like this:

      ```sh
      docker run -d --name redis_exporter -p 9121:9121 oliver006/redis_exporter
      ```

      Docker images are also published to the [quay.io docker repo](https://quay.io/oliver006/redis_exporter) so you can pull them from there if for instance you run into rate limiting issues with Docker hub.

      ```sh
      docker run -d --name redis_exporter -p 9121:9121 quay.io/oliver006/redis_exporter
      ```

      The `latest` docker image contains only the exporter binary.
      If e.g. for debugging purposes, you need the exporter running
      in an image that has a shell then you can run the `alpine` image:

      ```sh
      docker run -d --name redis_exporter -p 9121:9121 oliver006/redis_exporter:alpine
      ```

      If you try to access a Redis instance running on the host node, you'll need to add `--network host` so the
      redis_exporter container can access it:

      ```sh
      docker run -d --name redis_exporter --network host oliver006/redis_exporter
      ```

      ### Run on Kubernetes

      [Here](contrib/k8s-redis-and-exporter-deployment.yaml) is an example Kubernetes deployment configuration for how to deploy the redis_exporter as a sidecar to a Redis instance.


      ### Tile38

      [Tile38](https://tile38.com) now has native Prometheus support for exporting server metrics and basic stats about number of objects, strings, etc.
      You can also use redis_exporter to export Tile38 metrics, especially more advanced metrics by using Lua scripts or the `-check-keys` flag.\
      To enable Tile38 support, run the exporter with `--is-tile38=true`.


      ## What's exported

      Most items from the INFO command are exported,
      see [Redis documentation](https://redis.io/commands/info) for details.\
      In addition, for every database there are metrics for total keys, expiring keys and the average TTL for keys in the database.\
      You can also export values of keys by using the `-check-keys` (or related) flag. The exporter will also export the size (or, depending on the data type, the length) of the key.
      This can be used to export the number of elements in (sorted) sets, hashes, lists, streams, etc.
      If a key is in string format and matches with `--check-keys` (or related) then its string value will be exported as a label in the `key_value_as_string` metric.

      If you require custom metric collection, you can provide a [Redis Lua script](https://redis.io/commands/eval) using the `-script` flag. An example can be found [in the contrib folder](./contrib/sample_collect_script.lua).


      ### The redis_memory_max_bytes metric

      The metric `redis_memory_max_bytes`  will show the maximum number of bytes Redis can use.\
      It is zero if no memory limit is set for the Redis instance you're scraping (this is the default setting for Redis).\
      You can confirm that's the case by checking if the metric `redis_config_maxmemory` is zero or by connecting to the Redis instance via redis-cli and running the command `CONFIG GET MAXMEMORY`.


      ## What it looks like

      Example [Grafana](http://grafana.org/) screenshots:
      ![redis_exporter_screen_01](https://cloud.githubusercontent.com/assets/1222339/19412031/897549c6-92da-11e6-84a0-b091f9deb81d.png)

      ![redis_exporter_screen_02](https://cloud.githubusercontent.com/assets/1222339/19412041/dee6d7bc-92da-11e6-84f8-610c025d6182.png)

      Grafana dashboard is available on [grafana.com](https://grafana.com/dashboards/763) and/or [github.com](contrib/grafana_prometheus_redis_dashboard.json).

      ### Viewing multiple Redis simultaneously

      If running [Redis Sentinel](https://redis.io/topics/sentinel), it may be desirable to view the metrics of the various cluster members simultaneously. For this reason the dashboard's drop down is of the multi-value type, allowing for the selection of multiple Redis. Please note that there is a  caveat; the single stat panels up top namely `uptime`, `total memory use` and `clients` do not function upon viewing multiple Redis.


      ## Using the mixin
      There is a set of sample rules, alerts and dashboards available in [redis-mixin](contrib/redis-mixin/)

      ## Upgrading from 0.x to 1.x

      [PR #256](https://github.com/oliver006/redis_exporter/pull/256) introduced breaking changes which were released as version v1.0.0.

      If you only scrape one Redis instance and use command line flags `--redis.address`
      and `--redis.password` then you're most probably not affected.
      Otherwise, please see [PR #256](https://github.com/oliver006/redis_exporter/pull/256) and [this README](https://github.com/oliver006/redis_exporter#prometheus-configuration-to-scrape-multiple-redis-hosts) for more information.

      ## Memory Usage Aggregation by Key Groups

      When a single Redis instance is used for multiple purposes, it is useful to be able to see how Redis memory is consumed among the different usage scenarios. This is particularly important when a Redis instance with no eviction policy is running low on memory as we want to identify whether certain applications are misbehaving (e.g. not deleting keys that are no longer in use) or the Redis instance needs to be scaled up to handle the increased resource demand. Fortunately, most applications using Redis will employ some sort of naming conventions for keys tied to their specific purpose such as (hierarchical) namespace prefixes which can be exploited by the check-keys, check-single-keys, and count-keys parameters of redis_exporter to surface the memory usage metrics of specific scenarios. *Memory usage aggregation by key groups* takes this one step further by harnessing the flexibility of Redis LUA scripting support to classify all keys on a Redis instance into groups through a list of user-defined [LUA regular expressions](https://www.lua.org/pil/20.1.html) so memory usage metrics can be aggregated into readily identifiable groups.

      To enable memory usage aggregation by key groups, simply specify a non-empty comma-separated list of LUA regular expressions through the `check-key-groups` redis_exporter parameter. On each aggregation of memory metrics by key groups, redis_exporter will set up a `SCAN` cursor through all keys for each Redis database to be processed in batches via a LUA script. Each key batch is then processed by the same LUA script on a key-by-key basis as follows:

        1. The `MEMORY USAGE` command is called to gather memory usage for each key
        2. The specified LUA regexes are applied to each key in the specified order, and the group name that a given key belongs to will be derived from concatenating the capture groups of the first regex that matches the key. For example, applying the regex `^(.*)_[^_]+$` to the key `key_exp_Nick` would yield a group name of `key_exp`. If none of the specified regexes matches a key, the key will be assigned to the `unclassified` group

      Once a key has been classified, the memory usage and key counter for the corresponding group will be incremented in a local LUA table. This aggregated metrics table will then be returned alongside the next `SCAN` cursor position to redis_exporter when all keys in a batch have been processed, and redis_exporter can aggregate the data from all batches into a single table of grouped memory usage metrics for the Prometheus metrics scrapper.

      Besides making the full flexibility of LUA regex available for classifying keys into groups, the LUA script also has the benefit of reducing network traffic by executing all `MEMORY USAGE` commands on the Redis server and returning aggregated data to redis_exporter in a far more compact format than key-level data. The use of `SCAN` cursor over batches of keys processed by a server-side LUA script also helps prevent unbounded latency bubble in Redis's single processing thread, and the batch size can be tailored to specific environments via the `check-keys-batch-size` parameter.

      Scanning the entire key space of a Redis instance may sound a lttle extravagant, but it takes only a single scan to classify all keys into groups, and on a moderately sized system with ~780K keys and a rather complex list of 17 regexes, it takes an average of ~5s to perform a full aggregation of memory usage by key groups. Of course, the actual performance for specific systems will vary widely depending on the total number of keys, the number and complexity of regexes used for classification, and the configured batch size.

      To protect Prometheus from being overwhelmed by a large number of time series resulting from misconfigured group classification regular expression (e.g. applying the regular expression `^(.*)$` where each key will be classified into its own distinct group), a limit on the number of distinct key groups *per Redis database* can be configured via the `max-distinct-key-groups` parameter. If the `max-distinct-key-groups` limit is exceeded, only the key groups with the highest memory usage within the limit will be tracked separately, remaining key groups will be reported under a single `overflow` key group.

      Here is a list of additional metrics that will be exposed when memory usage aggregation by key groups is enabled:

      | Name                                               | Labels       | Description                                                                                   |
      |----------------------------------------------------|--------------|-----------------------------------------------------------------------------------------------|
      | redis_key_group_count                              | db,key_group | Number of keys in a key group                                                                 |
      | redis_key_group_memory_usage_bytes                 | db,key_group | Memory usage by key group                                                                     |
      | redis_number_of_distinct_key_groups                | db           | Number of distinct key groups in a Redis database when the `overflow` group is fully expanded |
      | redis_last_key_groups_scrape_duration_milliseconds |              | Duration of the last memory usage aggregation by key groups in milliseconds                   |

      ### Script to collect Redis lists and respective sizes.
      If using Redis version < 4.0, most of the helpful metrics which we need to gather based on length or memory is not possible via default redis_exporter.
      With the help of LUA scripts, we can gather these metrics.
      One of these scripts [contrib/collect_lists_length_growing.lua](./contrib/collect_lists_length_growing.lua) will help to collect the length of redis lists.
      With this count, we can take following actions such as Create alerts or dashboards in Grafana or any similar tools with these Prometheus metrics.

      ## Development

      The tests require a variety of real Redis instances to not only verify correctness of the exporter but also
      compatibility with older versions of Redis and with Redis-like systems like KeyDB or Tile38.\
      The [contrib/docker-compose-for-tests.yml](./contrib/docker-compose-for-tests.yml) file has service definitions for
      everything that's needed.\
      You can bring up the Redis test instances first by running `make docker-env-up` and then, every time you want to run the tests, you can run `make docker-test`. This will mount the current directory (with the .go source files) into a docker container and kick off the tests.\
      Once you're done testing you can bring down the stack by running `make docker-env-down`.\
      Or you can bring up the stack, run the tests, and then tear down the stack, all in one shot, by running `make docker-all`.

      ***Note.** Tests initialization can lead to unexpected results when using a persistent testing environment. When `make docker-env-up` is executed once and `make docker-test` is constantly run or stopped during execution, the number of keys in the database changes, which can lead to unexpected failures of tests. Use `make docker-env-down` periodacally to clean up as a workaround.*

      ## Communal effort

      Open an issue or PR if you have more suggestions, questions or ideas about what to add.
    ID: Unkonw
    arch: Unkonw
---
